{
  
    
        "post0": {
            "title": "A Simple Neural Network from Scratch with PyTorch and Google Colab",
            "content": "About . In this tutorial we will implement a simple neural network from scratch using PyTorch. The idea of the tutorial is to teach you the basics of PyTorch and how it can be used to implement a neural network from scratch. I will go over some of the basic functionalities and concepts available in PyTorch that will allow you to build your own neural networks. . This tutorial assumes you have prior knowledge of how a neural network works. Don’t worry! Even if you are not so sure, you will be okay. For advanced PyTorch users, this tutorial may still serve as a refresher. This tutorial is heavily inspired by this Neural Network implementation coded purely using Numpy. In fact, I tried re-implementing the code using PyTorch instead and added my own intuitions and explanations. Thanks to Samay for his phenomenal work, I hope this inspires many others as it did with me. . The torch module provides all the necessary tensor operators you will need to implement your first neural network from scratch in PyTorch. That&#39;s right! In PyTorch everything is a Tensor, so this is the first thing you will need to get used to. Let&#39;s import the libraries we will need for this tutorial. . import torch import torch.nn as nn . Data . Let&#39;s start by creating some sample data using the torch.tensor command. In Numpy, this could be done with np.array. Both functions serve the same purpose, but in PyTorch everything is a Tensor as opposed to a vector or matrix. We define types in PyTorch using the dtype=torch.xxx command. . In the data below, X represents the amount of hours studied and how much time students spent sleeping, whereas y represent grades. The variable xPredicted is a single input for which we want to predict a grade using the parameters learned by the neural network. Remember, the neural network wants to learn a mapping between X and y, so it will try to take a guess from what it has learned from the training data. . X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor xPredicted = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor . You can check the size of the tensors we have just created with the size command. This is equivalent to the shape command used in tools such as Numpy and Tensorflow. . print(X.size()) print(y.size()) . torch.Size([3, 2]) torch.Size([3, 1]) . Scaling . Below we are performing some scaling on the sample data. Notice that the max function returns both a tensor and the corresponding indices. So we use _ to capture the indices which we won&#39;t use here because we are only interested in the max values to conduct the scaling. Perfect! Our data is now in a very nice format our neural network will appreciate later on. . # scale units X_max, _ = torch.max(X, 0) xPredicted_max, _ = torch.max(xPredicted, 0) X = torch.div(X, X_max) xPredicted = torch.div(xPredicted, xPredicted_max) y = y / 100 # max test score is 100 print(xPredicted) . tensor([0.5000, 1.0000]) . Notice that there are two functions max and div that I didn&#39;t discuss above. They do exactly what they imply: max finds the maximum value in a vector... I mean tensor; and div is basically a nice little function to divide two tensors. . Model (Computation Graph) . Once the data has been processed and it is in the proper format, all you need to do now is to define your model. Here is where things begin to change a little as compared to how you would build your neural networks using, say, something like Keras or Tensorflow. However, you will realize quickly as you go along that PyTorch doesn&#39;t differ much from other deep learning tools. At the end of the day we are constructing a computation graph, which is used to dictate how data should flow and what type of operations are performed on this information. . For illustration purposes, we are building the following neural network or computation graph: . . class Neural_Network(nn.Module): def __init__(self, ): super(Neural_Network, self).__init__() # parameters # TODO: parameters can be parameterized instead of declaring them here self.inputSize = 2 self.outputSize = 1 self.hiddenSize = 3 # weights self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor def forward(self, X): self.z = torch.matmul(X, self.W1) # 3 X 3 &quot;.dot&quot; does not broadcast in PyTorch self.z2 = self.sigmoid(self.z) # activation function self.z3 = torch.matmul(self.z2, self.W2) o = self.sigmoid(self.z3) # final activation function return o def sigmoid(self, s): return 1 / (1 + torch.exp(-s)) def sigmoidPrime(self, s): # derivative of sigmoid return s * (1 - s) def backward(self, X, y, o): self.o_error = y - o # error in output self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2)) self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2) self.W1 += torch.matmul(torch.t(X), self.z2_delta) self.W2 += torch.matmul(torch.t(self.z2), self.o_delta) def train(self, X, y): # forward + backward pass for training o = self.forward(X) self.backward(X, y, o) def saveWeights(self, model): # we will use the PyTorch internal storage functions torch.save(model, &quot;NN&quot;) # you can reload model with all the weights and so forth with: # torch.load(&quot;NN&quot;) def predict(self): print (&quot;Predicted data based on trained weights: &quot;) print (&quot;Input (scaled): n&quot; + str(xPredicted)) print (&quot;Output: n&quot; + str(self.forward(xPredicted))) . For the purpose of this tutorial, we are not going to be talking math stuff, that&#39;s for another day. I just want you to get a gist of what it takes to build a neural network from scratch using PyTorch. Let&#39;s break down the model which was declared via the class above. . Class Header . First, we defined our model via a class because that is the recommended way to build the computation graph. The class header contains the name of the class Neural Network and the parameter nn.Module which basically indicates that we are defining our own neural network. . class Neural_Network(nn.Module): . Initialization . The next step is to define the initializations ( def __init__(self,)) that will be performed upon creating an instance of the customized neural network. You can declare the parameters of your model here, but typically, you would declare the structure of your network in this section -- the size of the hidden layers and so forth. Since we are building the neural network from scratch, we explicitly declared the size of the weights matrices: one that stores the parameters from the input to hidden layer; and one that stores the parameter from the hidden to output layer. Both weight matrices are initialized with values randomly chosen from a normal distribution via torch.randn(...). Note that we are not using bias just to keep things as simple as possible. . def __init__(self, ): super(Neural_Network, self).__init__() # parameters # TODO: parameters can be parameterized instead of declaring them here self.inputSize = 2 self.outputSize = 1 self.hiddenSize = 3 # weights self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor . The Forward Function . The forward function is where all the magic happens (see below). This is where the data enters and is fed into the computation graph (i.e., the neural network structure we have built). Since we are building a simple neural network with one hidden layer, our forward function looks very simple: . def forward(self, X): self.z = torch.matmul(X, self.W1) self.z2 = self.sigmoid(self.z) # activation function self.z3 = torch.matmul(self.z2, self.W2) o = self.sigmoid(self.z3) # final activation function return o . The forward function above takes the input Xand then performs a matrix multiplication (torch.matmul(...)) with the first weight matrix self.W1. Then the result is applied an activation function, sigmoid. The resulting matrix of the activation is then multiplied with the second weight matrix self.W2. Then another activation if performed, which renders the output of the neural network or computation graph. The process I described above is simply what&#39;s known as a feedforward pass. In order for the weights to optimize when training, we need a backpropagation algorithm. . The Backward Function . The backward function contains the backpropagation algorithm, where the goal is to essentially minimize the loss with respect to our weights. In other words, the weights need to be updated in such a way that the loss decreases while the neural network is training (well, that is what we hope for). All this magic is possible with the gradient descent algorithm which is declared in the backward function. Take a minute or two to inspect what is happening in the code below: . def backward(self, X, y, o): self.o_error = y - o # error in output self.o_delta = self.o_error * self.sigmoidPrime(o) self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2)) self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2) self.W1 += torch.matmul(torch.t(X), self.z2_delta) self.W2 += torch.matmul(torch.t(self.z2), self.o_delta) . Notice that we are performing a lot of matrix multiplications along with the transpose operations via the torch.matmul(...) and torch.t(...) operations, respectively. The rest is simply gradient descent -- there is nothing to it. . Training . All that is left now is to train the neural network. First we create an instance of the computation graph we have just built: . NN = Neural_Network() . Then we train the model for 1000 rounds. Notice that in PyTorch NN(X) automatically calls the forward function so there is no need to explicitly call NN.forward(X). . After we have obtained the predicted output for ever round of training, we compute the loss, with the following code: . torch.mean((y - NN(X))**2).detach().item() . The next step is to start the training (foward + backward) via NN.train(X, y). After we have trained the neural network, we can store the model and output the predicted value of the single instance we declared in the beginning, xPredicted. . Let&#39;s train! . NN = Neural_Network() for i in range(1000): # trains the NN 1,000 times if (i % 100) == 0: print (&quot;#&quot; + str(i) + &quot; Loss: &quot; + str(torch.mean((y - NN(X))**2).detach().item())) # mean sum squared loss NN.train(X, y) NN.saveWeights(NN) NN.predict() print(&quot;Finished training!&quot;) . #0 Loss: 0.24544493854045868 #100 Loss: 0.0026628002524375916 #200 Loss: 0.0024748605210334063 #300 Loss: 0.002363199135288596 #400 Loss: 0.0022466194350272417 #500 Loss: 0.0021235516760498285 #600 Loss: 0.001996910898014903 #700 Loss: 0.0018705682596191764 #800 Loss: 0.0017485078424215317 #900 Loss: 0.0016340742586180568 Predicted data based on trained weights: Input (scaled): tensor([0.5000, 1.0000]) Output: tensor([0.9529]) Finished training! . /usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn&#39;t retrieve source code for container of type Neural_Network. It won&#39;t be checked for correctness upon loading. &#34;type &#34; + obj.__name__ + &#34;. It won&#39;t be checked &#34; . The loss keeps decreasing, which means that the neural network is learning something. That&#39;s it. Congratulations! You have just learned how to create and train a neural network from scratch using PyTorch. There are so many things you can do with the shallow network we have just implemented. You can add more hidden layers or try to incorporate the bias terms for practice. I would love to see what you will build from here. Reach me out on Twitter if you have any further questions or leave your comments here. Until next time! . References: . PyTorch nn. Modules | Build a Neural Network with Numpy | .",
            "url": "https://dair-ai.github.io/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html",
            "relUrl": "/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "PyTorch 1.2 Quickstart with Google Colab",
            "content": "About . In this code tutorial we will learn how to quickly train a model to understand some of PyTorch&#39;s basic building blocks to train a deep learning model. This notebook is inspired by the &quot;Tensorflow 2.0 Quickstart for experts&quot; notebook. . After completion of this tutorial, you should be able to import data, transform it, and efficiently feed the data in batches to a convolution neural network (CNN) model for image classification. . Author: Elvis Saravia . ## import libraries import torch import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms . print(torch.__version__) . 1.4.0 . Import The Data . The first step before training the model is to import the data. We will use the MNIST dataset which is like the Hello World dataset of machine learning. . Besides importing the data, we will also do a few more things: . We will tranform the data into tensors using the transforms module | We will use DataLoader to build convenient data loaders or what are referred to as iterators, which makes it easy to efficiently feed data in batches to deep learning models. | As hinted above, we will also create batches of the data by setting the batch parameter inside the data loader. Notice we use batches of 32 in this tutorial but you can change it to 64 if you like. I encourage you to experiment with different batches. | . %%capture BATCH_SIZE = 32 ## transformations transform = transforms.Compose( [transforms.ToTensor()]) ## download and load training dataset trainset = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) ## download and load testing dataset testset = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2) . Exploring the Data . As a practioner and researcher, I am always spending a bit of time and effort exploring and understanding the dataset. It&#39;s fun and this is a good practise to ensure that everything is in order. . Let&#39;s check what the train and test dataset contains. I will use matplotlib to print out some of the images from our dataset. . import matplotlib.pyplot as plt import numpy as np ## functions to show an image def imshow(img): #img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) ## get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() ## show images imshow(torchvision.utils.make_grid(images)) . EXERCISE: Try to understand what the code above is doing. This will help you to better understand your dataset before moving forward. . Let&#39;s check the dimensions of a batch. . for images, labels in trainloader: print(&quot;Image batch dimensions:&quot;, images.shape) print(&quot;Image label dimensions:&quot;, labels.shape) break . Image batch dimensions: torch.Size([32, 1, 28, 28]) Image label dimensions: torch.Size([32]) . The Model . Now using the classical deep learning framework pipeline, let&#39;s build the 1 convolutional layer model. . Here are a few notes for those who are beginning with PyTorch: . The model below consists of an __init__() portion which is where you include the layers and components of the neural network. In our model, we have a convolutional layer denoted by nn.Conv2d(...). We are dealing with an image dataset that is in a grayscale so we only need one channel going in, hence in_channels=1. We hope to get a nice representation of this layer, so we use out_channels=32. Kernel size is 3, and for the rest of parameters we use the default values which you can find here. | We use 2 back to back dense layers or what we refer to as linear transformations to the incoming data. Notice for d1 I have a dimension which looks like it came out of nowhere. 128 represents the size we want as output and the (26*26*32) represents the dimension of the incoming data. If you would like to find out how to calculate those numbers refer to the PyTorch documentation. In short, the convolutional layer transforms the input data into a specific dimension that has to be considered in the linear layer. The same applies for the second linear transformation (d2) where the dimension of the output of the previous linear layer was added as in_features=128, and 10 is just the size of the output which also corresponds to the number of classes. | After each one of those layers, we also apply an activation function such as ReLU. For prediction purposes, we then apply a softmax layer to the last transformation and return the output of that. | . class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() # 28x28x1 =&gt; 26x26x32 self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3) self.d1 = nn.Linear(26 * 26 * 32, 128) self.d2 = nn.Linear(128, 10) def forward(self, x): # 32x1x28x28 =&gt; 32x32x26x26 x = self.conv1(x) x = F.relu(x) # flatten =&gt; 32 x (32*26*26) x = x.flatten(start_dim = 1) # 32 x (32*26*26) =&gt; 32x128 x = self.d1(x) x = F.relu(x) # logits =&gt; 32x10 logits = self.d2(x) out = F.softmax(logits, dim=1) return out . As I have done in my previous tutorials, I always encourage to test the model with 1 batch to ensure that the output dimensions are what we expect. . ## test the model with 1 batch model = MyModel() for images, labels in trainloader: print(&quot;batch size:&quot;, images.shape) out = model(images) print(out.shape) break . batch size: torch.Size([32, 1, 28, 28]) torch.Size([32, 10]) . Training the Model . Now we are ready to train the model but before that we are going to setup a loss function, an optimizer and a function to compute accuracy of the model. . learning_rate = 0.001 num_epochs = 5 device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model = MyModel() model = model.to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) . ## compute accuracy def get_accuracy(logit, target, batch_size): &#39;&#39;&#39; Obtain accuracy for training round &#39;&#39;&#39; corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum() accuracy = 100.0 * corrects/batch_size return accuracy.item() . Now it&#39;s time for training. . for epoch in range(num_epochs): train_running_loss = 0.0 train_acc = 0.0 model = model.train() ## training step for i, (images, labels) in enumerate(trainloader): images = images.to(device) labels = labels.to(device) ## forward + backprop + loss logits = model(images) loss = criterion(logits, labels) optimizer.zero_grad() loss.backward() ## update model params optimizer.step() train_running_loss += loss.detach().item() train_acc += get_accuracy(logits, labels, BATCH_SIZE) model.eval() print(&#39;Epoch: %d | Loss: %.4f | Train Accuracy: %.2f&#39; %(epoch, train_running_loss / i, train_acc/i)) . Epoch: 0 | Loss: 1.5831 | Train Accuracy: 88.24 Epoch: 1 | Loss: 1.4956 | Train Accuracy: 96.91 Epoch: 2 | Loss: 1.4834 | Train Accuracy: 98.03 Epoch: 3 | Loss: 1.4784 | Train Accuracy: 98.52 Epoch: 4 | Loss: 1.4751 | Train Accuracy: 98.81 . We can also compute accuracy on the testing dataset to see how well the model performs on the image classificaiton task. As you can see below, our basic CNN model is performing very well on the MNIST classification task. . test_acc = 0.0 for i, (images, labels) in enumerate(testloader, 0): images = images.to(device) labels = labels.to(device) outputs = model(images) test_acc += get_accuracy(outputs, labels, BATCH_SIZE) print(&#39;Test Accuracy: %.2f&#39;%( test_acc/i)) . Test Accuracy: 98.36 . EXERCISE: As a way to practise, try to include the testing part inside the code where I was outputing the training accuracy, so that you can also keep testing the model on the testing data as you proceed with the training steps. This is useful as sometimes you don&#39;t want to wait until your model has completed training to actually test the model with the testing data. . Final Words . That&#39;s it for this tutorial! Congratulations! You are now able to implement a basic CNN model in PyTorch for image classification. If you would like, you can further extend the CNN model by adding more convolution layers and max pooling, but as you saw, you don&#39;t really need it here as results look good. If you are interested in implementing a similar image classification model using RNNs see the references below. . References . Building RNNs is Fun with PyTorch and Google Colab | CNN Basics with PyTorch by Sebastian Raschka | Tensorflow 2.0 Quickstart for experts | .",
            "url": "https://dair-ai.github.io/notebooks/deep%20learning/cnn/neural%20network/image%20classification/intermediate/pytorch/2020/03/18/pytorch_quick_start.html",
            "relUrl": "/deep%20learning/cnn/neural%20network/image%20classification/intermediate/pytorch/2020/03/18/pytorch_quick_start.html",
            "date": " • Mar 18, 2020"
        }
        
    
  
    
  
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "dair.ai is a community effort to democratize Artificial Intelligence (AI) research, education, and technologies. . Notebooks is an effort to encourage data scientists from all levels to easily share their notebooks. .",
          "url": "https://dair-ai.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}