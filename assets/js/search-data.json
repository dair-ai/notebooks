{
  
    
        "post0": {
            "title": "A Simple Neural Network from Scratch with PyTorch and Google Colab",
            "content": "About . In this tutorial we will implement a simple neural network from scratch using PyTorch. The idea of the tutorial is to teach you the basics of PyTorch and how it can be used to implement a neural network from scratch. I will go over some of the basic functionalities and concepts available in PyTorch that will allow you to build your own neural networks. . This tutorial assumes you have prior knowledge of how a neural network works. Don’t worry! Even if you are not so sure, you will be okay. For advanced PyTorch users, this tutorial may still serve as a refresher. This tutorial is heavily inspired by this Neural Network implementation coded purely using Numpy. In fact, I tried re-implementing the code using PyTorch instead and added my own intuitions and explanations. Thanks to Samay for his phenomenal work, I hope this inspires many others as it did with me. . The torch module provides all the necessary tensor operators you will need to implement your first neural network from scratch in PyTorch. That&#39;s right! In PyTorch everything is a Tensor, so this is the first thing you will need to get used to. Let&#39;s import the libraries we will need for this tutorial. . import torch import torch.nn as nn . Data . Let&#39;s start by creating some sample data using the torch.tensor command. In Numpy, this could be done with np.array. Both functions serve the same purpose, but in PyTorch everything is a Tensor as opposed to a vector or matrix. We define types in PyTorch using the dtype=torch.xxx command. . In the data below, X represents the amount of hours studied and how much time students spent sleeping, whereas y represent grades. The variable xPredicted is a single input for which we want to predict a grade using the parameters learned by the neural network. Remember, the neural network wants to learn a mapping between X and y, so it will try to take a guess from what it has learned from the training data. . X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor xPredicted = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor . You can check the size of the tensors we have just created with the size command. This is equivalent to the shape command used in tools such as Numpy and Tensorflow. . print(X.size()) print(y.size()) . torch.Size([3, 2]) torch.Size([3, 1]) . Scaling . Below we are performing some scaling on the sample data. Notice that the max function returns both a tensor and the corresponding indices. So we use _ to capture the indices which we won&#39;t use here because we are only interested in the max values to conduct the scaling. Perfect! Our data is now in a very nice format our neural network will appreciate later on. . # scale units X_max, _ = torch.max(X, 0) xPredicted_max, _ = torch.max(xPredicted, 0) X = torch.div(X, X_max) xPredicted = torch.div(xPredicted, xPredicted_max) y = y / 100 # max test score is 100 print(xPredicted) . tensor([0.5000, 1.0000]) . Notice that there are two functions max and div that I didn&#39;t discuss above. They do exactly what they imply: max finds the maximum value in a vector... I mean tensor; and div is basically a nice little function to divide two tensors. . Model (Computation Graph) . Once the data has been processed and it is in the proper format, all you need to do now is to define your model. Here is where things begin to change a little as compared to how you would build your neural networks using, say, something like Keras or Tensorflow. However, you will realize quickly as you go along that PyTorch doesn&#39;t differ much from other deep learning tools. At the end of the day we are constructing a computation graph, which is used to dictate how data should flow and what type of operations are performed on this information. . For illustration purposes, we are building the following neural network or computation graph: . . class Neural_Network(nn.Module): def __init__(self, ): super(Neural_Network, self).__init__() # parameters # TODO: parameters can be parameterized instead of declaring them here self.inputSize = 2 self.outputSize = 1 self.hiddenSize = 3 # weights self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor def forward(self, X): self.z = torch.matmul(X, self.W1) # 3 X 3 &quot;.dot&quot; does not broadcast in PyTorch self.z2 = self.sigmoid(self.z) # activation function self.z3 = torch.matmul(self.z2, self.W2) o = self.sigmoid(self.z3) # final activation function return o def sigmoid(self, s): return 1 / (1 + torch.exp(-s)) def sigmoidPrime(self, s): # derivative of sigmoid return s * (1 - s) def backward(self, X, y, o): self.o_error = y - o # error in output self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2)) self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2) self.W1 += torch.matmul(torch.t(X), self.z2_delta) self.W2 += torch.matmul(torch.t(self.z2), self.o_delta) def train(self, X, y): # forward + backward pass for training o = self.forward(X) self.backward(X, y, o) def saveWeights(self, model): # we will use the PyTorch internal storage functions torch.save(model, &quot;NN&quot;) # you can reload model with all the weights and so forth with: # torch.load(&quot;NN&quot;) def predict(self): print (&quot;Predicted data based on trained weights: &quot;) print (&quot;Input (scaled): n&quot; + str(xPredicted)) print (&quot;Output: n&quot; + str(self.forward(xPredicted))) . For the purpose of this tutorial, we are not going to be talking math stuff, that&#39;s for another day. I just want you to get a gist of what it takes to build a neural network from scratch using PyTorch. Let&#39;s break down the model which was declared via the class above. . Class Header . First, we defined our model via a class because that is the recommended way to build the computation graph. The class header contains the name of the class Neural Network and the parameter nn.Module which basically indicates that we are defining our own neural network. . class Neural_Network(nn.Module): . Initialization . The next step is to define the initializations ( def __init__(self,)) that will be performed upon creating an instance of the customized neural network. You can declare the parameters of your model here, but typically, you would declare the structure of your network in this section -- the size of the hidden layers and so forth. Since we are building the neural network from scratch, we explicitly declared the size of the weights matrices: one that stores the parameters from the input to hidden layer; and one that stores the parameter from the hidden to output layer. Both weight matrices are initialized with values randomly chosen from a normal distribution via torch.randn(...). Note that we are not using bias just to keep things as simple as possible. . def __init__(self, ): super(Neural_Network, self).__init__() # parameters # TODO: parameters can be parameterized instead of declaring them here self.inputSize = 2 self.outputSize = 1 self.hiddenSize = 3 # weights self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor . The Forward Function . The forward function is where all the magic happens (see below). This is where the data enters and is fed into the computation graph (i.e., the neural network structure we have built). Since we are building a simple neural network with one hidden layer, our forward function looks very simple: . def forward(self, X): self.z = torch.matmul(X, self.W1) self.z2 = self.sigmoid(self.z) # activation function self.z3 = torch.matmul(self.z2, self.W2) o = self.sigmoid(self.z3) # final activation function return o . The forward function above takes the input Xand then performs a matrix multiplication (torch.matmul(...)) with the first weight matrix self.W1. Then the result is applied an activation function, sigmoid. The resulting matrix of the activation is then multiplied with the second weight matrix self.W2. Then another activation if performed, which renders the output of the neural network or computation graph. The process I described above is simply what&#39;s known as a feedforward pass. In order for the weights to optimize when training, we need a backpropagation algorithm. . The Backward Function . The backward function contains the backpropagation algorithm, where the goal is to essentially minimize the loss with respect to our weights. In other words, the weights need to be updated in such a way that the loss decreases while the neural network is training (well, that is what we hope for). All this magic is possible with the gradient descent algorithm which is declared in the backward function. Take a minute or two to inspect what is happening in the code below: . def backward(self, X, y, o): self.o_error = y - o # error in output self.o_delta = self.o_error * self.sigmoidPrime(o) self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2)) self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2) self.W1 += torch.matmul(torch.t(X), self.z2_delta) self.W2 += torch.matmul(torch.t(self.z2), self.o_delta) . Notice that we are performing a lot of matrix multiplications along with the transpose operations via the torch.matmul(...) and torch.t(...) operations, respectively. The rest is simply gradient descent -- there is nothing to it. . Training . All that is left now is to train the neural network. First we create an instance of the computation graph we have just built: . NN = Neural_Network() . Then we train the model for 1000 rounds. Notice that in PyTorch NN(X) automatically calls the forward function so there is no need to explicitly call NN.forward(X). . After we have obtained the predicted output for ever round of training, we compute the loss, with the following code: . torch.mean((y - NN(X))**2).detach().item() . The next step is to start the training (foward + backward) via NN.train(X, y). After we have trained the neural network, we can store the model and output the predicted value of the single instance we declared in the beginning, xPredicted. . Let&#39;s train! . NN = Neural_Network() for i in range(1000): # trains the NN 1,000 times if (i % 100) == 0: print (&quot;#&quot; + str(i) + &quot; Loss: &quot; + str(torch.mean((y - NN(X))**2).detach().item())) # mean sum squared loss NN.train(X, y) NN.saveWeights(NN) NN.predict() print(&quot;Finished training!&quot;) . #0 Loss: 0.24544493854045868 #100 Loss: 0.0026628002524375916 #200 Loss: 0.0024748605210334063 #300 Loss: 0.002363199135288596 #400 Loss: 0.0022466194350272417 #500 Loss: 0.0021235516760498285 #600 Loss: 0.001996910898014903 #700 Loss: 0.0018705682596191764 #800 Loss: 0.0017485078424215317 #900 Loss: 0.0016340742586180568 Predicted data based on trained weights: Input (scaled): tensor([0.5000, 1.0000]) Output: tensor([0.9529]) Finished training! . /usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn&#39;t retrieve source code for container of type Neural_Network. It won&#39;t be checked for correctness upon loading. &#34;type &#34; + obj.__name__ + &#34;. It won&#39;t be checked &#34; . The loss keeps decreasing, which means that the neural network is learning something. That&#39;s it. Congratulations! You have just learned how to create and train a neural network from scratch using PyTorch. There are so many things you can do with the shallow network we have just implemented. You can add more hidden layers or try to incorporate the bias terms for practice. I would love to see what you will build from here. Reach me out on Twitter if you have any further questions or leave your comments here. Until next time! . References: . PyTorch nn. Modules | Build a Neural Network with Numpy | .",
            "url": "https://dair-ai.github.io/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html",
            "relUrl": "/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "",
            "content": "Introduction . Author: Elvis Saravia ( Twitter | LinkedIn) . The full project will be maintained here. . . . Natural language processing (NLP) has made substantial advances in the past few years due to the success of modern techniques that are based on deep learning. With the rise of the popularity of NLP and the availability of different forms of large-scale data, it is now even more imperative to understand the inner workings of NLP techniques and concepts, from first principles, as they find their way into real-world usage and applications that affect society at large. Building intuitions and having a solid grasp of concepts are both important for coming up with innovative techniques, improving research, and building safe, human-centered AI and NLP technologies. . In this first chapter, which is part of a series called Fundamentals of NLP, we will learn about some of the most important basic concepts that power NLP techniques used for research and building real-world applications. Some of these techniques include lemmatization, stemming, tokenization, and sentence segmentation. These are all important techniques to train efficient and effective NLP models. Along the way, we will also cover best practices and common mistakes to avoid when training and building NLP models. We also provide some exercises for you to keep practicing and exploring some ideas. . In every chapter, we will introduce the theoretical aspect and motivation of each concept covered. Then we will obtain hands-on experience by using bootstrap methods, industry-standard tools, and other open-source libraries to implement the different techniques. Along the way, we will also cover best practices, share important references, point out common mistakes to avoid when training and building NLP models, and discuss what lies ahead. . . Tokenization . With any typical NLP task, one of the first steps is to tokenize your pieces of text into its individual words/tokens (process demonstrated in the figure above), the result of which is used to create so-called vocabularies that will be used in the langauge model you plan to build. This is actually one of the techniques that we will use the most throughout this series but here we stick to the basics. . Below I am showing you an example of a simple tokenizer without any following any standards. All it does is extract tokens based on a white space seperator. . Try to running the following code blocks. . ## required libraries that need to be installed %%capture !pip install -U spacy !pip install -U spacy-lookups-data !python -m spacy download en_core_web_sm . ## tokenizing a piecen of text doc = &quot;I love coding and writing&quot; for i, w in enumerate(doc.split(&quot; &quot;)): print(&quot;Token &quot; + str(i) + &quot;: &quot; + w) . Token 0: I Token 1: love Token 2: coding Token 3: and Token 4: writing . All the code does is separate the sentence into individual tokens. The above simple block of code works well on the text I have provided. But typically, text is a lot noisier and complex than the example I used. For instance, if I used the word &quot;so-called&quot; is that one word or two words? For such scenarios, you may need more advanced approaches for tokenization. You can consider stripping away the &quot;-&quot; and splitting into two tokens or just combining into one token but this all depends on the problem and domain you are working on. . Another problem with our simple algorithm is that it cannot deal with extra whitespaces in the text. In addition, how do we deal with cities like &quot;New York&quot; and &quot;San Francisco&quot;? . . Exercise 1: Copy the code from above and add extra whitespaces to the string value assigned to the doc variable and identify the issue with the code. Then try to fix the issue. Hint: Use text.strip() to fix the problem. . ### ENTER CODE HERE ### . . Tokenization can also come in different forms. For instance, more recently a lot of state-of-the-art NLP models such as BERT make use of subword tokens in which frequent combinations of characters also form part of the vocabulary. This helps to deal with the so-called out of vocabulary (OOV) problem. We will discuss this in upcoming chapters, but if you are interested in reading more about this now, check this paper. . To demonstrate how you can achieve more reliable tokenization, we are going to use spaCy, which is an impressive and robust Python library for natural language processing. In particular, we are going to use the built-in tokenizer found here. . Run the code block below. . ## import the libraries import spacy ## load the language model nlp = spacy.load(&quot;en_core_web_sm&quot;) ## tokenization doc = nlp(&quot;This is the so-called lemmatization&quot;) for token in doc: print(token.text) . This is the so - called lemmatization . All the code does is tokenize the text based on a pre-built language model. . Try putting different running text into the nlp() part of the code above. The tokenizer is quiet robust and it includes a series of built-in rules that deal with exceptions and special cases such as those tokens that contain puctuations like &quot;`&quot; and &quot;.&quot;, &quot;-&quot;, etc. You can even add your own rules, find out how here. . In a later chapter of the series, we will do a deep dive on tokenization and the different tools that exist out there that can simplify and speed up the process of tokenization to build vocabularies. Some of the tools we will explore are the Keras Tokenizer API and Hugging Face Tokenizer. . . Lemmatization . Lemmatization is the process where we take individual tokens from a sentence and we try to reduce them to their base form. The process that makes this possible is having a vocabulary and performing morphological analysis to remove inflectional endings. The output of the lemmatization process (as shown in the figure above) is the lemma or the base form of the word. For instance, a lemmatization process reduces the inflections, &quot;am&quot;, &quot;are&quot;, and &quot;is&quot;, to the base form, &quot;be&quot;. Take a look at the figure above for a full example and try to understand what it&#39;s doing. . Lemmatization is helpful for normalizing text for text classification tasks or search engines, and a variety of other NLP tasks such as sentiment classification. It is particularly important when dealing with complex languages like Arabic and Spanish. . To show how you can achieve lemmatization and how it works, we are going to use spaCy again. Using the spaCy Lemmatizer class, we are going to convert a few words into their lemmas. . Below I show an example of how to lemmatize a sentence using spaCy. Try to run the block of code below and inspect the results. . ## import the libraries from spacy.lemmatizer import Lemmatizer from spacy.lookups import Lookups ## lemmatization doc = nlp(u&#39;I love coding and writing&#39;) for word in doc: print(word.text, &quot;=&gt;&quot;, word.lemma_) . I =&gt; -PRON- love =&gt; love coding =&gt; code and =&gt; and writing =&gt; writing . The results above look as expected. The only lemma that looks off is the -PRON- returned for the &quot;I&quot; token. According to the spaCy documentation, &quot;This is in fact expected behavior and not a bug. Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, -PRON-, which is used as the lemma for all personal pronouns.&quot; . Check out more about this in the spaCy documentation. . . Exercise 2: Try the code above with different sentences and see if you get any unexpected results. Also, try adding punctuations and extra whitespaces which are more common in natural language. What happens? . ### ENTER CODE HERE ### . . We can also create our own custom lemmatizer as shown below (code adopted directly from the spaCy website): . ## lookup tables lookups = Lookups() lookups.add_table(&quot;lemma_rules&quot;, {&quot;noun&quot;: [[&quot;s&quot;, &quot;&quot;]]}) lemmatizer = Lemmatizer(lookups) words_to_lemmatize = [&quot;cats&quot;, &quot;brings&quot;, &quot;sings&quot;] for w in words_to_lemmatize: lemma = lemmatizer(w, &quot;NOUN&quot;) print(lemma) . [&#39;cat&#39;] [&#39;bring&#39;] [&#39;sing&#39;] . In the example code above, we added one lemma rule, which aims to identify plural nouns and remove the plurality, i.e. remove the &quot;s&quot;. There are different types of rules you can add here. I encourage you to head over to the spaCy documentation to learn a bit more. . . Stemming . Stemming is just a simpler version of lemmatization where we are interested in stripping the suffix at the end of the word. When stemming we are interesting in reducing the inflected or derived word to it&#39;s base form. Take a look at the figure above to get some intuition about the process. . Both the stemming and the lemmatization processes involve morphological analysis) where the stems and affixes (called the morphemes) are extracted and used to reduce inflections to their base form. For instance, the word cats has two morphemes, cat and s, the cat being the stem and the s being the affix representing plurality. . spaCy doesn&#39;t support stemming so for this part we are going to use NLTK, which is another fantastic Python NLP library. . The simple example below demonstrates how you can stem words in a piece of text. Go ahead and run the code to see what happens. . from nltk.stem.snowball import SnowballStemmer stemmer = SnowballStemmer(language=&#39;english&#39;) doc = &#39;I prefer not to argue&#39; for token in doc.split(&quot; &quot;): print(token, &#39;=&gt;&#39; , stemmer.stem(token)) . I =&gt; i prefer =&gt; prefer not =&gt; not to =&gt; to =&gt; argue =&gt; argu . Notice how the stemmed version of the word &quot;argue&quot; is &quot;argu&quot;. That&#39;s because we can have derived words like &quot;argument&quot;, &quot;arguing&quot;, and &quot;argued&quot;. . . Exercise 3: Try to use different sentences in the code above and observe the effect of the stemmer. By the way, there are other stemmers such as the Porter stemmer in the NLTK library. Each stemmer behaves differently so the output may vary. Feel free to try the Porter stemmer from the NLTK library and inspect the output of the different stemmers. . ### ENTER CODE HERE ### . . Sentence Segmentation . When dealing with text, it is always common that we need to break up text into its individual sentences. That is what is known as sentence segmentation: the process of obtaining the individual sentences from a text corpus. The resulting segments can then be analyzed individually with the techniques that we previously learned. . In the spaCy library, we have the choice to use a built-in sentence segmenter (trained on statistical models) or build your own rule-based method. In fact, we will cover a few examples to demonstrate the difficultiness of this problem. . Below I created a naive implementation of a sentence segmentation algorithm without using any kind of special library. You can see that my code increases with complexity (bugs included) as I start to consider more rules. This sort of boostrapped or rule-based approach is sometimes your only option depending on the language you are working with or the availability of linguistic resources. . Run the code below to apply a simple algorithm for sentence segmentation. . ## using a simple rule-based segmenter with native python code text = &quot;I love coding and programming. I also love sleeping!&quot; current_position = 0 cursor = 0 sentences = [] for c in text: if c == &quot;.&quot; or c == &quot;!&quot;: sentences.append(text[current_position:cursor+1]) current_position = cursor + 2 cursor+=1 print(sentences) . [&#39;I love coding and programming.&#39;, &#39;I also love sleeping!&#39;] . Our sentence segmenter only segments sentences when it meets a sentence boundary which in this case is either a &quot;.&quot; or a &quot;!&quot;. It&#39;s not the cleanest of code but it shows how difficult the task can get as we are presented with richer text that include more diverse special characters. One problem with my code is that I am not able to differentiate between abbreviations like Dr. and numbers like 0.4. You may be able to create your own complex regular expression (we will get into this in the second chapter) to deal with these special cases but it still requires a lot of work and debugging. Luckily for us, there are libraries like spaCy and NLTK which help with this sort of preprocessing tasks. . Let&#39;s try the sentence segmentation provided by spaCy. Run the code below and inspect the results. . doc = nlp(&quot;I love coding and programming. I also love sleeping!&quot;) for sent in doc.sents: print(sent.text) . I love coding and programming. I also love sleeping! . Here is a link showing how you can create your own rule-based strategy for sentence segmentation using spaCy. This is particulary useful if you are working with domain-specific text which is full with noisy information and is not as standardized as text found on a factual Wiki page or news website. . . Exercise 4: For practise, try to create your own sentence segmentation algorithm using spaCy (try this link for help and ideas). At this point, I am encouraging you to look at documentation which is a huge part of learning in-depth about all the concepts we will cover in this series. Research is a huge part of the learning process. . ### ENTER CODE HERE ### . . How to use with Machine Learning? . When you are working with textual information, it is imperative to clean your data so as to be able to train more accurate machine learning (ML) models. . One of the reasons why transformations like lemmatization and stemming are useful is for normalizing the text before you feed the output to an ML algorithm. For instance, if you are building a sentiment analysis model how can you tell the model that &quot;smiling&quot; and &quot;smile&quot; refer to the same concept? You may require stemming if you are using TF-IDF features combined with a machine learning algorithm such as Naive Bayes classifier. As you may suspect already, this also requires a really good tokenizer to come up with the features, especially when work on noisy pieces of text that could be generated from users in a social media site. . With a wide variety of NLP tasks, one of the first big steps in the NLP pipeline is to create a vocabulary that will eventually be used to determine the inputs for the model representing the features. In modern NLP techniques such as pretrained language models, you need to process a text corpus that require proper and more sophisticated sentence segmentation and tokenization as we discussed before. We will talk more about these methods in due time. For now, the basics presented here are a good start into the world of practical NLP. Spend some time reading up on all the concepts mentioned here and take notes. I will guide through the series on what are the important parts and provide you with relevant links but you can also conduct your own additional research on the side and even improve this notebook. . Final Words and What&#39;s Next? . In this chapter we learned some fundamental concepts of NLP such as lemmatization, stemming, sentence segmentations, and tokenization. In the next chapter we will cover topics such as word normalization, regular expressions, part of speech and edit distance, all very important topics when working with information retrieval and NLP systems. . References . Model Languages from spaCyu | Speech and Language Processing - Jurafsky and Martin | Python for NLP: Tokenization, Stemming, and Lemmatization with SpaCy Library | Stemming | Lemmatizer | Stanford IR Book | Linguistic Features by spaCy | .",
            "url": "https://dair-ai.github.io/notebooks/2020/03/18/2020-03-19-nlp_basics_tokenization_segmentation.html",
            "relUrl": "/2020/03/18/2020-03-19-nlp_basics_tokenization_segmentation.html",
            "date": " • Mar 18, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "PyTorch 1.2 Quickstart with Google Colab",
            "content": "About . In this code tutorial we will learn how to quickly train a model to understand some of PyTorch&#39;s basic building blocks to train a deep learning model. This notebook is inspired by the &quot;Tensorflow 2.0 Quickstart for experts&quot; notebook. . After completion of this tutorial, you should be able to import data, transform it, and efficiently feed the data in batches to a convolution neural network (CNN) model for image classification. . Author: Elvis Saravia . ## import libraries import torch import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms . print(torch.__version__) . 1.4.0 . Import The Data . The first step before training the model is to import the data. We will use the MNIST dataset which is like the Hello World dataset of machine learning. . Besides importing the data, we will also do a few more things: . We will tranform the data into tensors using the transforms module | We will use DataLoader to build convenient data loaders or what are referred to as iterators, which makes it easy to efficiently feed data in batches to deep learning models. | As hinted above, we will also create batches of the data by setting the batch parameter inside the data loader. Notice we use batches of 32 in this tutorial but you can change it to 64 if you like. I encourage you to experiment with different batches. | . %%capture BATCH_SIZE = 32 ## transformations transform = transforms.Compose( [transforms.ToTensor()]) ## download and load training dataset trainset = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) ## download and load testing dataset testset = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2) . Exploring the Data . As a practioner and researcher, I am always spending a bit of time and effort exploring and understanding the dataset. It&#39;s fun and this is a good practise to ensure that everything is in order. . Let&#39;s check what the train and test dataset contains. I will use matplotlib to print out some of the images from our dataset. . import matplotlib.pyplot as plt import numpy as np ## functions to show an image def imshow(img): #img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) ## get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() ## show images imshow(torchvision.utils.make_grid(images)) . EXERCISE: Try to understand what the code above is doing. This will help you to better understand your dataset before moving forward. . Let&#39;s check the dimensions of a batch. . for images, labels in trainloader: print(&quot;Image batch dimensions:&quot;, images.shape) print(&quot;Image label dimensions:&quot;, labels.shape) break . Image batch dimensions: torch.Size([32, 1, 28, 28]) Image label dimensions: torch.Size([32]) . The Model . Now using the classical deep learning framework pipeline, let&#39;s build the 1 convolutional layer model. . Here are a few notes for those who are beginning with PyTorch: . The model below consists of an __init__() portion which is where you include the layers and components of the neural network. In our model, we have a convolutional layer denoted by nn.Conv2d(...). We are dealing with an image dataset that is in a grayscale so we only need one channel going in, hence in_channels=1. We hope to get a nice representation of this layer, so we use out_channels=32. Kernel size is 3, and for the rest of parameters we use the default values which you can find here. | We use 2 back to back dense layers or what we refer to as linear transformations to the incoming data. Notice for d1 I have a dimension which looks like it came out of nowhere. 128 represents the size we want as output and the (26*26*32) represents the dimension of the incoming data. If you would like to find out how to calculate those numbers refer to the PyTorch documentation. In short, the convolutional layer transforms the input data into a specific dimension that has to be considered in the linear layer. The same applies for the second linear transformation (d2) where the dimension of the output of the previous linear layer was added as in_features=128, and 10 is just the size of the output which also corresponds to the number of classes. | After each one of those layers, we also apply an activation function such as ReLU. For prediction purposes, we then apply a softmax layer to the last transformation and return the output of that. | . class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() # 28x28x1 =&gt; 26x26x32 self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3) self.d1 = nn.Linear(26 * 26 * 32, 128) self.d2 = nn.Linear(128, 10) def forward(self, x): # 32x1x28x28 =&gt; 32x32x26x26 x = self.conv1(x) x = F.relu(x) # flatten =&gt; 32 x (32*26*26) x = x.flatten(start_dim = 1) # 32 x (32*26*26) =&gt; 32x128 x = self.d1(x) x = F.relu(x) # logits =&gt; 32x10 logits = self.d2(x) out = F.softmax(logits, dim=1) return out . As I have done in my previous tutorials, I always encourage to test the model with 1 batch to ensure that the output dimensions are what we expect. . ## test the model with 1 batch model = MyModel() for images, labels in trainloader: print(&quot;batch size:&quot;, images.shape) out = model(images) print(out.shape) break . batch size: torch.Size([32, 1, 28, 28]) torch.Size([32, 10]) . Training the Model . Now we are ready to train the model but before that we are going to setup a loss function, an optimizer and a function to compute accuracy of the model. . learning_rate = 0.001 num_epochs = 5 device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model = MyModel() model = model.to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) . ## compute accuracy def get_accuracy(logit, target, batch_size): &#39;&#39;&#39; Obtain accuracy for training round &#39;&#39;&#39; corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum() accuracy = 100.0 * corrects/batch_size return accuracy.item() . Now it&#39;s time for training. . for epoch in range(num_epochs): train_running_loss = 0.0 train_acc = 0.0 model = model.train() ## training step for i, (images, labels) in enumerate(trainloader): images = images.to(device) labels = labels.to(device) ## forward + backprop + loss logits = model(images) loss = criterion(logits, labels) optimizer.zero_grad() loss.backward() ## update model params optimizer.step() train_running_loss += loss.detach().item() train_acc += get_accuracy(logits, labels, BATCH_SIZE) model.eval() print(&#39;Epoch: %d | Loss: %.4f | Train Accuracy: %.2f&#39; %(epoch, train_running_loss / i, train_acc/i)) . Epoch: 0 | Loss: 1.5831 | Train Accuracy: 88.24 Epoch: 1 | Loss: 1.4956 | Train Accuracy: 96.91 Epoch: 2 | Loss: 1.4834 | Train Accuracy: 98.03 Epoch: 3 | Loss: 1.4784 | Train Accuracy: 98.52 Epoch: 4 | Loss: 1.4751 | Train Accuracy: 98.81 . We can also compute accuracy on the testing dataset to see how well the model performs on the image classificaiton task. As you can see below, our basic CNN model is performing very well on the MNIST classification task. . test_acc = 0.0 for i, (images, labels) in enumerate(testloader, 0): images = images.to(device) labels = labels.to(device) outputs = model(images) test_acc += get_accuracy(outputs, labels, BATCH_SIZE) print(&#39;Test Accuracy: %.2f&#39;%( test_acc/i)) . Test Accuracy: 98.36 . EXERCISE: As a way to practise, try to include the testing part inside the code where I was outputing the training accuracy, so that you can also keep testing the model on the testing data as you proceed with the training steps. This is useful as sometimes you don&#39;t want to wait until your model has completed training to actually test the model with the testing data. . Final Words . That&#39;s it for this tutorial! Congratulations! You are now able to implement a basic CNN model in PyTorch for image classification. If you would like, you can further extend the CNN model by adding more convolution layers and max pooling, but as you saw, you don&#39;t really need it here as results look good. If you are interested in implementing a similar image classification model using RNNs see the references below. . References . Building RNNs is Fun with PyTorch and Google Colab | CNN Basics with PyTorch by Sebastian Raschka | Tensorflow 2.0 Quickstart for experts | .",
            "url": "https://dair-ai.github.io/notebooks/deep%20learning/cnn/neural%20network/image%20classification/intermediate/pytorch/2020/03/18/pytorch_quick_start.html",
            "relUrl": "/deep%20learning/cnn/neural%20network/image%20classification/intermediate/pytorch/2020/03/18/pytorch_quick_start.html",
            "date": " • Mar 18, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Implementing A Logistic Regression Model from Scratch with PyTorch",
            "content": "About . In this tutorial, we are going to implement a logistic regression model from scratch with PyTorch. The model will be designed with neural networks in mind and will be used for a simple image classification task. I believe this is a great approach to begin understanding the fundamental building blocks behind a neural network. Additionally, we will also look at best practices on how to use PyTorch for training neural networks. . After completing this tutorial the learner is expected to know the basic building blocks of a logistic regression model. The learner is also expected to apply the logistic regression model to a binary image classification problem of their choice using PyTorch code. . . Author: Elvis Saravia ( Twitter | LinkedIn) . Complete Code Walkthrough: Blog post . ## Import the usual libraries import torch import torchvision import torch.nn as nn from torchvision import datasets, models, transforms import os import numpy as np import matplotlib.pyplot as plt %matplotlib inline ## print out the pytorch version used (1.31 at the time of this tutorial) print(torch.__version__) . 1.3.1 . ## configuration to detect cuda or cpu device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) print (device) . cuda:0 . Importing Dataset . In this tutorial we will be working on an image classification problem. You can find the public dataset here. . The objective of our model is to learn to classify between &quot;bee&quot; vs. &quot;no bee&quot; images. . Since we are using Google Colab, we will need to first import our data into our environment using the code below: . ## importing dataset from google.colab import drive drive.mount(&#39;gdrive&#39;, force_remount=True) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly Enter your authorization code: ·········· Mounted at gdrive . Data Transformation . This is an image classification task, which means that we need to perform a few transformations on our dataset before we train our models. I used similar transformations as used in this tutorial. For a detailed overview of each transformation take a look at the official torchvision documentation. . The following code block performs the following operations: . The data_transforms contains a series of transformations that will be performed on each image found in the dataset. This includes cropping the image, resizing the image, converting it to tensor, reshaping it, and normalizing it. | Once those transformations have been defined, then the DataLoader function is used to automatically load the datasets and perform any additional configuration such as shuffling, batches, etc. | . ## configure root folder on your gdrive data_dir = &#39;gdrive/My Drive/DAIR RESOURCES/TF to PT/datasets/hymenoptera_data&#39; ## custom transformer to flatten the image tensors class ReshapeTransform: def __init__(self, new_size): self.new_size = new_size def __call__(self, img): result = torch.reshape(img, self.new_size) return result ## transformations used to standardize and normalize the datasets data_transforms = { &#39;train&#39;: transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(), ReshapeTransform((-1,)) # flattens the data ]), &#39;val&#39;: transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(), ReshapeTransform((-1,)) # flattens the data ]), } ## load the correspoding folders image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [&#39;train&#39;, &#39;val&#39;]} ## load the entire dataset; we are not using minibatches here train_dataset = torch.utils.data.DataLoader(image_datasets[&#39;train&#39;], batch_size=len(image_datasets[&#39;train&#39;]), shuffle=True) test_dataset = torch.utils.data.DataLoader(image_datasets[&#39;val&#39;], batch_size=len(image_datasets[&#39;val&#39;]), shuffle=True) . Print sample . It&#39;s always a good practise to take a quick look at the dataset before training your models. Below we print out an example of one of the images from the train_dataset. . ## load the entire dataset x, y = next(iter(train_dataset)) ## print one example dim = x.shape[1] print(&quot;Dimension of image:&quot;, x.shape, &quot; n&quot;, &quot;Dimension of labels&quot;, y.shape) plt.imshow(x[160].reshape(1, 3, 224, 224).squeeze().T.numpy()) . Dimension of image: torch.Size([244, 150528]) Dimension of labels torch.Size([244]) . &lt;matplotlib.image.AxesImage at 0x7fcab44f1c88&gt; . Building the Model . Let&#39;s now implement our logistic regression model. Logistic regression is one in a family of machine learning techniques that are used to train binary classifiers. They are also a great way to understand the fundamental building blocks of neural networks, thus they can also be considered the simplest of neural networks where the model performs a forward and backward propagation to train the model on the data provided. . If you don&#39;t fully understand the structure of the code below, I strongly recommend you to read the following tutorial, which I wrote for PyTorch beginners. You can also check out Week 2 of Andrew Ng&#39;s Deep Learning Specialization course for all the explanation, intuitions, and details of the different parts of the neural network such as the forward, sigmoid, backward, and optimization steps. . In short: . The __init__ function initializes all the parameters (W, b, grad) that will be used to train the model through backpropagation. | The goal is to learn the W and b that minimimizes the cost function which is computed as seen in the loss function below. | . Note that this is a very detailed implementation of a logistic regression model so I had to explicitly move a lot of the computations into the GPU for faster calcuation, to(device) takes care of this in PyTorch. . class LR(nn.Module): def __init__(self, dim, lr=torch.scalar_tensor(0.01)): super(LR, self).__init__() # intialize parameters self.w = torch.zeros(dim, 1, dtype=torch.float).to(device) self.b = torch.scalar_tensor(0).to(device) self.grads = {&quot;dw&quot;: torch.zeros(dim, 1, dtype=torch.float).to(device), &quot;db&quot;: torch.scalar_tensor(0).to(device)} self.lr = lr.to(device) def forward(self, x): ## compute forward z = torch.mm(self.w.T, x) a = self.sigmoid(z) return a def sigmoid(self, z): return 1/(1 + torch.exp(-z)) def backward(self, x, yhat, y): ## compute backward self.grads[&quot;dw&quot;] = (1/x.shape[1]) * torch.mm(x, (yhat - y).T) self.grads[&quot;db&quot;] = (1/x.shape[1]) * torch.sum(yhat - y) def optimize(self): ## optimization step self.w = self.w - self.lr * self.grads[&quot;dw&quot;] self.b = self.b - self.lr * self.grads[&quot;db&quot;] ## utility functions def loss(yhat, y): m = y.size()[1] return -(1/m)* torch.sum(y*torch.log(yhat) + (1 - y)* torch.log(1-yhat)) def predict(yhat, y): y_prediction = torch.zeros(1, y.size()[1]) for i in range(yhat.size()[1]): if yhat[0, i] &lt;= 0.5: y_prediction[0, i] = 0 else: y_prediction[0, i] = 1 return 100 - torch.mean(torch.abs(y_prediction - y)) * 100 . Pretesting the Model . It is also good practice to test your model and make sure the right steps are taking place before training the entire model. . ## model pretesting x, y = next(iter(train_dataset)) ## flatten/transform the data x_flatten = x.T y = y.unsqueeze(0) ## num_px is the dimension of the images dim = x_flatten.shape[0] ## model instance model = LR(dim) model.to(device) yhat = model.forward(x_flatten.to(device)) yhat = yhat.data.cpu() ## calculate loss cost = loss(yhat, y) prediction = predict(yhat, y) print(&quot;Cost: &quot;, cost) print(&quot;Accuracy: &quot;, prediction) ## backpropagate model.backward(x_flatten.to(device), yhat.to(device), y.to(device)) model.optimize() . Cost: tensor(0.6931) Accuracy: tensor(50.4098) . Train the Model . It&#39;s now time to train the model. . ## hyperparams costs = [] dim = x_flatten.shape[0] learning_rate = torch.scalar_tensor(0.0001).to(device) num_iterations = 100 lrmodel = LR(dim, learning_rate) lrmodel.to(device) ## transform the data def transform_data(x, y): x_flatten = x.T y = y.unsqueeze(0) return x_flatten, y ## training the model for i in range(num_iterations): x, y = next(iter(train_dataset)) test_x, test_y = next(iter(test_dataset)) x, y = transform_data(x, y) test_x, test_y = transform_data(test_x, test_y) # forward yhat = lrmodel.forward(x.to(device)) cost = loss(yhat.data.cpu(), y) train_pred = predict(yhat, y) # backward lrmodel.backward(x.to(device), yhat.to(device), y.to(device)) lrmodel.optimize() ## test yhat_test = lrmodel.forward(test_x.to(device)) test_pred = predict(yhat_test, test_y) if i % 10 == 0: costs.append(cost) if i % 10 == 0: print(&quot;Cost after iteration {}: {} | Train Acc: {} | Test Acc: {}&quot;.format(i, cost, train_pred, test_pred)) . Cost after iteration 0: 0.6931470036506653 | Train Acc: 50.40983581542969 | Test Acc: 45.75163269042969 Cost after iteration 10: 0.6691471934318542 | Train Acc: 64.3442611694336 | Test Acc: 54.24836730957031 Cost after iteration 20: 0.6513187885284424 | Train Acc: 68.44261932373047 | Test Acc: 54.24836730957031 Cost after iteration 30: 0.6367831230163574 | Train Acc: 68.03278350830078 | Test Acc: 54.24836730957031 Cost after iteration 40: 0.6245343685150146 | Train Acc: 69.67213439941406 | Test Acc: 54.90196228027344 Cost after iteration 50: 0.6139233112335205 | Train Acc: 70.90164184570312 | Test Acc: 56.20914840698242 Cost after iteration 60: 0.6045243740081787 | Train Acc: 72.54098510742188 | Test Acc: 56.86274337768555 Cost after iteration 70: 0.5960519909858704 | Train Acc: 74.18032836914062 | Test Acc: 57.51633834838867 Cost after iteration 80: 0.5883094668388367 | Train Acc: 73.77049255371094 | Test Acc: 57.51633834838867 Cost after iteration 90: 0.581156849861145 | Train Acc: 74.59016418457031 | Test Acc: 58.1699333190918 . Result . From the loss curve below you can see that the model is sort of learning to classify the images given the decreas in the loss. I only ran the model for 100 iterations. Train the model for many more rounds and analyze the results. In fact, I have suggested a couple of experiments and exercises at the end of the tutorial that you can try to get a more improved model. . ## the trend in the context of loss plt.plot(costs) plt.show() . Exercises . There are many improvements and different experiments that you can perform on top of this notebook to keep practising ML: . It is always good to normalize/standardize your images which helps with learning. As an experiment, you can research and try different ways to standarize the dataset. We have normalized the dataset with the builtin PyTorch normalizer which uses the mean and standard deviation. Alternatively, you can simply divide the original pixel values by 255 which is what a lot of ML engineers do. Play around with this idea, and try different transformation or normalization techniques. What effect does this have on learning in terms of speed and loss? | The dataset is too small so our model is not really learning effectively. You can try many things to help with learning such as playing around with the learning rate. Try to decrease and increase the learning rate and observe the effect of this in learning? | If you explored the dataset further, you may have noticed that all the &quot;no-bee&quot; images are actually &quot;ant&quot; images. If you would like to create a more robust model, you may want to make your &quot;no-bee&quot; images more random and diverse. Additionally, the dataset is also being shuffled which you can easily disable in the data transformation section. What happens if you disable the shuffling? | The model is not really performing that well because of the dataset I am using and because I didn&#39;t train it for long enough. It is a relatively small dataset but the performance should get better with more training over time. A more challenging task involves adopting the model to other datasets. Give it a try! | Another important part that is missing in this tutorial is the comprehensive analysis of the model results. If you understand the code, it should be easy to figure out how to test with a few examples. In fact, it would also be great if you can put aside a small testing dataset for this part of the exercise, so as to test the generalization capabilities of the model. | We built the logistic regression model from scratch but with libraries like PyTorch, these days you can simply leverage the high-level functions that implement certain parts of the neural network for you. This simplifies your code and minimizes the amount of bugs in your code. Plus you don&#39;t have to code your neural networks from scratch all the time. As a bonus exercise, try to adapt PyTorch builtin modules and functions for implementing a simpler version of the above logistic regression model. I will also add this as a to-do task for myself and post a solution soon. | . References . Understanding the Impact of Learning Rate on Neural Network Performance | TRANSFER LEARNING FOR COMPUTER VISION TUTORIAL | Deep Learning Specialization by Andrew Ng | .",
            "url": "https://dair-ai.github.io/notebooks/machine%20learning/beginner/logistic%20regression/2020/03/18/pytorch_logistic_regression.html",
            "relUrl": "/machine%20learning/beginner/logistic%20regression/2020/03/18/pytorch_logistic_regression.html",
            "date": " • Mar 18, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A First Shot at Deep Learning with PyTorch",
            "content": "About . In this notebook, we are going to take a baby step into the world of deep learning using PyTorch. There are a ton of notebooks out there that teach you the fundamentals of deep learning and PyTorch, so here the idea is to give you some basic introduction to deep learning and PyTorch at a very high level. Therefore, this notebook is targeting beginners but it can also serve as a review for more experienced developers. . After completion of this notebook, you are expected to know the basic components of training a basic neural network with PyTorch. I have also left a couple of exercises towards the end with the intention of encouraging more research and practise of your deep learning skills. . . Author: Elvis Saravia - Twitter | LinkedIn . Complete Code Walkthrough: Blog post . Importing the libraries . Like with any other programming exercise, the first step is to import the necessary libraries. As we are going to be using Google Colab to program our neural network, we need to install and import the necessary PyTorch libraries. . !pip3 install torch torchvision . Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0) Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0) Requirement already satisfied: pillow&gt;=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0) . ## The usual imports import torch import torch.nn as nn ## print out the pytorch version used print(torch.__version__) . 1.4.0 . The Neural Network . Before building and training a neural network the first step is to process and prepare the data. In this notebook, we are going to use syntethic data (i.e., fake data) so we won&#39;t be using any real world data. . For the sake of simplicity, we are going to use the following input and output pairs converted to tensors, which is how data is typically represented in the world of deep learning. The x values represent the input of dimension (6,1) and the y values represent the output of similar dimension. The example is taken from this tutorial. . The objective of the neural network model that we are going to build and train is to automatically learn patterns that better characterize the relationship between the x and y values. Essentially, the model learns the relationship that exists between inputs and outputs which can then be used to predict the corresponding y value for any given input x. . ## our data in tensor form x = torch.tensor([[-1.0], [0.0], [1.0], [2.0], [3.0], [4.0]], dtype=torch.float) y = torch.tensor([[-3.0], [-1.0], [1.0], [3.0], [5.0], [7.0]], dtype=torch.float) . ## print size of the input tensor x.size() . torch.Size([6, 1]) . The Neural Network Components . As said earlier, we are going to first define and build out the components of our neural network before training the model. . Model . Typically, when building a neural network model, we define the layers and weights which form the basic components of the model. Below we show an example of how to define a hidden layer named layer1 with size (1, 1). For the purpose of this tutorial, we won&#39;t explicitly define the weights and allow the built-in functions provided by PyTorch to handle that part for us. By the way, the nn.Linear(...) function applies a linear transformation ($y = xA^T + b$) to the data that was provided as its input. We ignore the bias for now by setting bias=False. . ## Neural network with 1 hidden layer layer1 = nn.Linear(1,1, bias=False) model = nn.Sequential(layer1) . Loss and Optimizer . The loss function, nn.MSELoss(), is in charge of letting the model know how good it has learned the relationship between the input and output. The optimizer (in this case an SGD) primary role is to minimize or lower that loss value as it tunes its weights. . ## loss function criterion = nn.MSELoss() ## optimizer algorithm optimizer = torch.optim.SGD(model.parameters(), lr=0.01) . Training the Neural Network Model . We have all the components we need to train our model. Below is the code used to train our model. . In simple terms, we train the model by feeding it the input and output pairs for a couple of rounds (i.e., epoch). After a series of forward and backward steps, the model somewhat learns the relationship between x and y values. This is notable by the decrease in the computed loss. For a more detailed explanation of this code check out this tutorial. . ## training for i in range(150): model = model.train() ## forward output = model(x) loss = criterion(output, y) optimizer.zero_grad() ## backward + update model params loss.backward() optimizer.step() model.eval() print(&#39;Epoch: %d | Loss: %.4f&#39; %(i, loss.detach().item())) . Epoch: 0 | Loss: 25.5853 Epoch: 1 | Loss: 20.6815 Epoch: 2 | Loss: 16.7388 Epoch: 3 | Loss: 13.5688 Epoch: 4 | Loss: 11.0201 Epoch: 5 | Loss: 8.9709 Epoch: 6 | Loss: 7.3234 Epoch: 7 | Loss: 5.9987 Epoch: 8 | Loss: 4.9337 Epoch: 9 | Loss: 4.0774 Epoch: 10 | Loss: 3.3889 Epoch: 11 | Loss: 2.8353 Epoch: 12 | Loss: 2.3903 Epoch: 13 | Loss: 2.0325 Epoch: 14 | Loss: 1.7448 Epoch: 15 | Loss: 1.5134 Epoch: 16 | Loss: 1.3275 Epoch: 17 | Loss: 1.1779 Epoch: 18 | Loss: 1.0577 Epoch: 19 | Loss: 0.9610 Epoch: 20 | Loss: 0.8833 Epoch: 21 | Loss: 0.8208 Epoch: 22 | Loss: 0.7706 Epoch: 23 | Loss: 0.7302 Epoch: 24 | Loss: 0.6977 Epoch: 25 | Loss: 0.6716 Epoch: 26 | Loss: 0.6506 Epoch: 27 | Loss: 0.6338 Epoch: 28 | Loss: 0.6202 Epoch: 29 | Loss: 0.6093 Epoch: 30 | Loss: 0.6005 Epoch: 31 | Loss: 0.5935 Epoch: 32 | Loss: 0.5878 Epoch: 33 | Loss: 0.5832 Epoch: 34 | Loss: 0.5796 Epoch: 35 | Loss: 0.5766 Epoch: 36 | Loss: 0.5742 Epoch: 37 | Loss: 0.5723 Epoch: 38 | Loss: 0.5708 Epoch: 39 | Loss: 0.5696 Epoch: 40 | Loss: 0.5686 Epoch: 41 | Loss: 0.5678 Epoch: 42 | Loss: 0.5671 Epoch: 43 | Loss: 0.5666 Epoch: 44 | Loss: 0.5662 Epoch: 45 | Loss: 0.5659 Epoch: 46 | Loss: 0.5656 Epoch: 47 | Loss: 0.5654 Epoch: 48 | Loss: 0.5652 Epoch: 49 | Loss: 0.5651 Epoch: 50 | Loss: 0.5650 Epoch: 51 | Loss: 0.5649 Epoch: 52 | Loss: 0.5648 Epoch: 53 | Loss: 0.5648 Epoch: 54 | Loss: 0.5647 Epoch: 55 | Loss: 0.5647 Epoch: 56 | Loss: 0.5646 Epoch: 57 | Loss: 0.5646 Epoch: 58 | Loss: 0.5646 Epoch: 59 | Loss: 0.5646 Epoch: 60 | Loss: 0.5646 Epoch: 61 | Loss: 0.5646 Epoch: 62 | Loss: 0.5645 Epoch: 63 | Loss: 0.5645 Epoch: 64 | Loss: 0.5645 Epoch: 65 | Loss: 0.5645 Epoch: 66 | Loss: 0.5645 Epoch: 67 | Loss: 0.5645 Epoch: 68 | Loss: 0.5645 Epoch: 69 | Loss: 0.5645 Epoch: 70 | Loss: 0.5645 Epoch: 71 | Loss: 0.5645 Epoch: 72 | Loss: 0.5645 Epoch: 73 | Loss: 0.5645 Epoch: 74 | Loss: 0.5645 Epoch: 75 | Loss: 0.5645 Epoch: 76 | Loss: 0.5645 Epoch: 77 | Loss: 0.5645 Epoch: 78 | Loss: 0.5645 Epoch: 79 | Loss: 0.5645 Epoch: 80 | Loss: 0.5645 Epoch: 81 | Loss: 0.5645 Epoch: 82 | Loss: 0.5645 Epoch: 83 | Loss: 0.5645 Epoch: 84 | Loss: 0.5645 Epoch: 85 | Loss: 0.5645 Epoch: 86 | Loss: 0.5645 Epoch: 87 | Loss: 0.5645 Epoch: 88 | Loss: 0.5645 Epoch: 89 | Loss: 0.5645 Epoch: 90 | Loss: 0.5645 Epoch: 91 | Loss: 0.5645 Epoch: 92 | Loss: 0.5645 Epoch: 93 | Loss: 0.5645 Epoch: 94 | Loss: 0.5645 Epoch: 95 | Loss: 0.5645 Epoch: 96 | Loss: 0.5645 Epoch: 97 | Loss: 0.5645 Epoch: 98 | Loss: 0.5645 Epoch: 99 | Loss: 0.5645 Epoch: 100 | Loss: 0.5645 Epoch: 101 | Loss: 0.5645 Epoch: 102 | Loss: 0.5645 Epoch: 103 | Loss: 0.5645 Epoch: 104 | Loss: 0.5645 Epoch: 105 | Loss: 0.5645 Epoch: 106 | Loss: 0.5645 Epoch: 107 | Loss: 0.5645 Epoch: 108 | Loss: 0.5645 Epoch: 109 | Loss: 0.5645 Epoch: 110 | Loss: 0.5645 Epoch: 111 | Loss: 0.5645 Epoch: 112 | Loss: 0.5645 Epoch: 113 | Loss: 0.5645 Epoch: 114 | Loss: 0.5645 Epoch: 115 | Loss: 0.5645 Epoch: 116 | Loss: 0.5645 Epoch: 117 | Loss: 0.5645 Epoch: 118 | Loss: 0.5645 Epoch: 119 | Loss: 0.5645 Epoch: 120 | Loss: 0.5645 Epoch: 121 | Loss: 0.5645 Epoch: 122 | Loss: 0.5645 Epoch: 123 | Loss: 0.5645 Epoch: 124 | Loss: 0.5645 Epoch: 125 | Loss: 0.5645 Epoch: 126 | Loss: 0.5645 Epoch: 127 | Loss: 0.5645 Epoch: 128 | Loss: 0.5645 Epoch: 129 | Loss: 0.5645 Epoch: 130 | Loss: 0.5645 Epoch: 131 | Loss: 0.5645 Epoch: 132 | Loss: 0.5645 Epoch: 133 | Loss: 0.5645 Epoch: 134 | Loss: 0.5645 Epoch: 135 | Loss: 0.5645 Epoch: 136 | Loss: 0.5645 Epoch: 137 | Loss: 0.5645 Epoch: 138 | Loss: 0.5645 Epoch: 139 | Loss: 0.5645 Epoch: 140 | Loss: 0.5645 Epoch: 141 | Loss: 0.5645 Epoch: 142 | Loss: 0.5645 Epoch: 143 | Loss: 0.5645 Epoch: 144 | Loss: 0.5645 Epoch: 145 | Loss: 0.5645 Epoch: 146 | Loss: 0.5645 Epoch: 147 | Loss: 0.5645 Epoch: 148 | Loss: 0.5645 Epoch: 149 | Loss: 0.5645 . Testing the Model . After training the model we have the ability to test the model predictive capability by passing it an input. Below is a simple example of how you could achieve this with our model. The result we obtained aligns with the results obtained in this notebook, which inspired this entire tutorial. . ## test the model sample = torch.tensor([10.0], dtype=torch.float) predicted = model(sample) print(predicted.detach().item()) . 17.096769332885742 . Final Words . Congratulations! In this tutorial you learned how to train a simple neural network using PyTorch. You also learned about the basic components that make up a neural network model such as the linear transformation layer, optimizer, and loss function. We then trained the model and tested its predictive capabilities. You are well on your way to become more knowledgeable about deep learning and PyTorch. I have provided a bunch of references below if you are interested in practising and learning more. . I would like to thank Laurence Moroney for his excellent tutorial which I used as an inspiration for this tutorial. . Exercises . Add more examples in the input and output tensors. In addition, try to change the dimensions of the data, say by adding an extra value in each array. What needs to be changed to successfully train the network with the new data? | The model converged really fast, which means it learned the relationship between x and y values after a couple of iterations. Do you think it makes sense to continue training? How would you automate the process of stopping the training after the model loss doesn&#39;t subtantially change? | In our example, we used a single hidden layer. Try to take a look at the PyTorch documentation to figure out what you need to do to get a model with more layers. What happens if you add more hidden layers? | We did not discuss the learning rate (lr-0.001) and the optimizer in great detail. Check out the PyTorch documentation to learn more about what other optimizers you can use. | . References . The Hello World of Deep Learning with Neural Networks | A Simple Neural Network from Scratch with PyTorch and Google Colab | PyTorch Official Docs | PyTorch 1.2 Quickstart with Google Colab | A Gentle Intoduction to PyTorch | .",
            "url": "https://dair-ai.github.io/notebooks/deep%20learning/beginner/neural%20network/2020/03/18/pytorch_hello_world.html",
            "relUrl": "/deep%20learning/beginner/neural%20network/2020/03/18/pytorch_hello_world.html",
            "date": " • Mar 18, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Building RNNs is Fun with PyTorch and Google Colab",
            "content": "About . In this tutorial, I will first teach you how to build a recurrent neural network (RNN) with a single layer, consisting of one single neuron, with PyTorch and Google Colab. I will also show you how to implement a simple RNN-based model for image classification. . This work is heavily inspired by Aurélien Géron&#39;s book called &quot;Hand-On Machine Learning with Scikit-Learn and TensorFlow&quot;. Although his neural network implementations are purely in TensorFlow, I adopted/reused some notations/variables names and implemented things using PyTorch only. I really enjoyed his book and learned a lot from his explanations. His work inspired this tutorial and I strongly recommend the book. . We first import the necessary libraries we will use in the tutorial: . import torch import torch.nn as nn import torch.nn.functional as F import os import numpy as np . RNN with A Single Neuron . The idea of this tutorial is to show you the basic operations necessary for building an RNN architecture using PyTorch. This guide assumes you have knowledge of basic RNNs and that you have read the tutorial on building neural networks from scratch using PyTorch. I will try to review RNNs wherever possible for those that need a refresher but I will keep it minimal. . First, let&#39;s build the computation graph for a single-layer RNN. Again, we are not concerned with the math for now, I just want to show you the PyTorch operations needed to build your RNN models. . For illustration purposes, this is the architecture we are building: . . And here is the code: . class SingleRNN(nn.Module): def __init__(self, n_inputs, n_neurons): super(SingleRNN, self).__init__() self.Wx = torch.randn(n_inputs, n_neurons) # 4 X 1 self.Wy = torch.randn(n_neurons, n_neurons) # 1 X 1 self.b = torch.zeros(1, n_neurons) # 1 X 4 def forward(self, X0, X1): self.Y0 = torch.tanh(torch.mm(X0, self.Wx) + self.b) # 4 X 1 self.Y1 = torch.tanh(torch.mm(self.Y0, self.Wy) + torch.mm(X1, self.Wx) + self.b) # 4 X 1 return self.Y0, self.Y1 . In the above code, I have implemented a simple one layer, one neuron RNN. I initialized two weight matrices, Wx and Wy with values from a normal distribution. Wx contains connection weights for the inputs of the current time step, while Wy contains connection weights for the outputs of the previous time step. We added a bias b. The forward function computes two outputs -- one for each time step... two in this case. Note that we are using tanh as the nonlinearity (activation function). . As for the input, we are providing 4 instances, with each instance containing two input sequences. . For illustration purposes, this is how the data is being fed into the RNN model: . . And this is the code to test the model: . N_INPUT = 4 N_NEURONS = 1 X0_batch = torch.tensor([[0,1,2,0], [3,4,5,0], [6,7,8,0], [9,0,1,0]], dtype = torch.float) #t=0 =&gt; 4 X 4 X1_batch = torch.tensor([[9,8,7,0], [0,0,0,0], [6,5,4,0], [3,2,1,0]], dtype = torch.float) #t=1 =&gt; 4 X 4 model = SingleRNN(N_INPUT, N_NEURONS) Y0_val, Y1_val = model(X0_batch, X1_batch) . After we have fed the input into the computation graph, we obtain outputs for each timestep (Y0, Y1), which we can now print as follows: . print(Y0_val) print(Y1_val) . tensor([[-0.1643], [-0.9995], [-1.0000], [-1.0000]]) tensor([[-1.0000], [-0.6354], [-1.0000], [-0.9998]]) . Increasing Neurons in RNN Layer . Next, I will show you how to generalize the RNN we have just build to let the single layer support an n amount of neurons. In terms of the architecture, nothing really changes since we have already parameterized the number of neurons in the computation graph we have built. However, the size of the output changes since we have changed the size of number of units (i.e., neurons) in the RNN layer.  . Here is an illustration of what we will build: . . And here is the code: . class BasicRNN(nn.Module): def __init__(self, n_inputs, n_neurons): super(BasicRNN, self).__init__() self.Wx = torch.randn(n_inputs, n_neurons) # n_inputs X n_neurons self.Wy = torch.randn(n_neurons, n_neurons) # n_neurons X n_neurons self.b = torch.zeros(1, n_neurons) # 1 X n_neurons def forward(self, X0, X1): self.Y0 = torch.tanh(torch.mm(X0, self.Wx) + self.b) # batch_size X n_neurons self.Y1 = torch.tanh(torch.mm(self.Y0, self.Wy) + torch.mm(X1, self.Wx) + self.b) # batch_size X n_neurons return self.Y0, self.Y1 . N_INPUT = 3 # number of features in input N_NEURONS = 5 # number of units in layer X0_batch = torch.tensor([[0,1,2], [3,4,5], [6,7,8], [9,0,1]], dtype = torch.float) #t=0 =&gt; 4 X 3 X1_batch = torch.tensor([[9,8,7], [0,0,0], [6,5,4], [3,2,1]], dtype = torch.float) #t=1 =&gt; 4 X 3 model = BasicRNN(N_INPUT, N_NEURONS) Y0_val, Y1_val = model(X0_batch, X1_batch) . Now when we print the outputs produced for each time step, it is of size (4 X 5), which represents the batch size and number of neurons, respectively. . print(Y0_val) print(Y1_val) . tensor([[ 0.9975, -0.9785, 0.9822, -0.8972, 0.9929], [ 0.9999, -0.9998, 1.0000, -0.9865, 0.9447], [ 1.0000, -1.0000, 1.0000, -0.9983, 0.6298], [-1.0000, 0.9915, 0.7409, 1.0000, -1.0000]]) tensor([[ 0.9858, -1.0000, 1.0000, -0.8826, -1.0000], [ 0.1480, -0.8635, -0.4498, 0.3516, -0.2848], [-0.1455, -0.9988, 1.0000, 0.0260, -0.9997], [-0.4084, 0.9973, 0.8858, 0.0783, -0.9993]]) . PyTorch Built-in RNN Cell . If you take a closer look at the BasicRNN computation graph we have just built, it has a serious flaw. What if we wanted to build an architecture that supports extremely large inputs and outputs. The way it is currently built, it would require us to individually compute the outputs for every time step, increasing the lines of code needed to implement the desired computation graph. Below I will show you how to consolidate and implement this more efficiently and cleanly using the built-in RNNCell module. . Let&#39;s first try to implement this informally to analyze the role RNNCell plays: . rnn = nn.RNNCell(3, 5) # n_input X n_neurons X_batch = torch.tensor([[[0,1,2], [3,4,5], [6,7,8], [9,0,1]], [[9,8,7], [0,0,0], [6,5,4], [3,2,1]] ], dtype = torch.float) # X0 and X1 hx = torch.randn(4, 5) # m X n_neurons output = [] # for each time step for i in range(2): hx = rnn(X_batch[i], hx) output.append(hx) print(output) . [tensor([[ 0.2545, 0.7355, 0.3708, -0.6381, 0.0402], [-0.3379, 0.9996, 0.9976, -0.9769, 0.6668], [-0.9940, 1.0000, 1.0000, -0.9992, 0.4488], [-0.7486, 0.9925, 0.9862, -0.9642, 0.9990]], grad_fn=&lt;TanhBackward&gt;), tensor([[-0.9848, 1.0000, 1.0000, -0.9999, 0.9970], [ 0.2496, -0.7512, 0.1730, -0.3533, -0.7347], [-0.9502, 0.9998, 0.9995, -0.9966, 0.9119], [-0.6488, 0.7944, 0.9580, -0.9171, 0.2384]], grad_fn=&lt;TanhBackward&gt;)] . With the above code, we have basically implemented the same model that was implemented in BasicRNN. torch.RNNCell(...) does all the magic of creating and maintaining the necessary weights and biases for us. torch.RNNCell accepts a tensor as input and outputs the next hidden state for each element in the batch. Read more about this module here. . Now, let&#39;s formally build the computation graph using the same information we used above. . class CleanBasicRNN(nn.Module): def __init__(self, batch_size, n_inputs, n_neurons): super(CleanBasicRNN, self).__init__() self.rnn = nn.RNNCell(n_inputs, n_neurons) self.hx = torch.randn(batch_size, n_neurons) # initialize hidden state def forward(self, X): output = [] # for each time step for i in range(2): self.hx = self.rnn(X[i], self.hx) output.append(self.hx) return output, self.hx . FIXED_BATCH_SIZE = 4 # our batch size is fixed for now N_INPUT = 3 N_NEURONS = 5 X_batch = torch.tensor([[[0,1,2], [3,4,5], [6,7,8], [9,0,1]], [[9,8,7], [0,0,0], [6,5,4], [3,2,1]] ], dtype = torch.float) # X0 and X1 model = CleanBasicRNN(FIXED_BATCH_SIZE, N_INPUT, N_NEURONS) output_val, states_val = model(X_batch) print(output_val) # contains all output for all timesteps print(states_val) # contain values for final state or final timestep, i.e., t=1 . [tensor([[ 0.4582, -0.9106, 0.0743, -0.9608, 0.9272], [ 0.2087, -0.9999, -0.9486, -0.9969, 0.9996], [-0.2371, -1.0000, -0.7662, -1.0000, 1.0000], [-0.9576, -0.9306, -0.1201, -0.9781, 0.9277]], grad_fn=&lt;TanhBackward&gt;), tensor([[-0.9237, -1.0000, -0.9743, -1.0000, 1.0000], [-0.3181, -0.6270, -0.6122, 0.1921, 0.0647], [-0.7835, -0.9991, -0.9098, -0.9999, 0.9976], [-0.5765, -0.8469, -0.5469, -0.9785, 0.7512]], grad_fn=&lt;TanhBackward&gt;)] tensor([[-0.9237, -1.0000, -0.9743, -1.0000, 1.0000], [-0.3181, -0.6270, -0.6122, 0.1921, 0.0647], [-0.7835, -0.9991, -0.9098, -0.9999, 0.9976], [-0.5765, -0.8469, -0.5469, -0.9785, 0.7512]], grad_fn=&lt;TanhBackward&gt;) . You can see how the code is much cleaner since we don&#39;t need to explicitly operate on the weights as shown in the previous code snippet  --  everything is handled implicitly and eloquently behind the scenes by PyTorch. . RNN for Image Classification . . Now that you have learned how to build a simple RNN from scratch and using the built-in RNNCell module provided in PyTorch, let&#39;s do something more sophisticated and special. . Let&#39;s try to build an image classifier using the MNIST dataset. The MNIST dataset consists of images that contain hand-written numbers from 1–10. Essentially, we want to build a classifier to predict the numbers displayed by a set of images. I know this sounds strange but you will be surprised by how well RNNs perform on this image classification task. . In addition, we will also be using the RNN module instead of the RNNCell module since we want to generalize the computation graph to be able to support an n number of layers as well. We will only use one layer in the following computation graph, but you can experiment with the code later on by adding more layers. . Importing the dataset . Before building the RNN-based computation graph, let&#39;s import the MNIST dataset, split it into test and train portions, do a few transformations, and further explore it. You will need the following PyTorch libraries and lines of code to download and import the MNIST dataset to Google Colab. . import torchvision import torchvision.transforms as transforms . %%capture BATCH_SIZE = 64 # list all transformations transform = transforms.Compose( [transforms.ToTensor()]) # download and load training dataset trainset = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) # download and load testing dataset testset = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2) . The code above loads and prepares the dataset to be fed into the computation graph we will build later on. Take a few minutes to play around with the code and understand what is happening. Notice that we needed to provide a batch size. This is because trainloader and testloader are iterators which will make it easier when we are iterating on the dataset and training our RNN model with minibatches. . Exloring the dataset . Here is a few lines of code to explore the dataset. I won&#39;t cover much of what&#39;s going on here, but you can take some time and look at it by yourself. . import matplotlib.pyplot as plt import numpy as np # functions to show an image def imshow(img): #img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) # get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() # show images imshow(torchvision.utils.make_grid(images)) . Model . Let&#39;s construct the computation graph. Below are the parameters: . # parameters N_STEPS = 28 N_INPUTS = 28 N_NEURONS = 150 N_OUTPUTS = 10 N_EPHOCS = 10 . And finally, here is a figure of the RNN-based classification model we are building: . . And here is the code for the model: . class ImageRNN(nn.Module): def __init__(self, batch_size, n_steps, n_inputs, n_neurons, n_outputs): super(ImageRNN, self).__init__() self.n_neurons = n_neurons self.batch_size = batch_size self.n_steps = n_steps self.n_inputs = n_inputs self.n_outputs = n_outputs self.basic_rnn = nn.RNN(self.n_inputs, self.n_neurons) self.FC = nn.Linear(self.n_neurons, self.n_outputs) def init_hidden(self,): # (num_layers, batch_size, n_neurons) return (torch.zeros(1, self.batch_size, self.n_neurons)) def forward(self, X): # transforms X to dimensions: n_steps X batch_size X n_inputs X = X.permute(1, 0, 2) self.batch_size = X.size(1) self.hidden = self.init_hidden() # lstm_out =&gt; n_steps, batch_size, n_neurons (hidden states for each time step) # self.hidden =&gt; 1, batch_size, n_neurons (final state from each lstm_out) lstm_out, self.hidden = self.basic_rnn(X, self.hidden) out = self.FC(self.hidden) return out.view(-1, self.n_outputs) # batch_size X n_output . The ImageRNN model is doing the following: . The initialization function __init__(...) declares a few variables, and then a basic RNN layer basic_rnn followed by a fully-connected layer self.FC. | The init_hidden function initializes hidden weights with zero values. The forward function accepts an input of size n_steps X batch_size X n_neurons. Then the data flows through the RNN layer and then through the fully-connected layer. | The output are the log probabilities of the model. | . Testing the model with some samples . A very good practice encouraged by PyTorch developers throughout their documentation, and which I really like and highly recommend, is to always test the model with a portion of the dataset before actual training. This is to ensure that you have the correct dimension specified and that the model is outputing the information you expect. Below I show an example of how to test your model: . dataiter = iter(trainloader) images, labels = dataiter.next() model = ImageRNN(BATCH_SIZE, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS) logits = model(images.view(-1, 28,28)) print(logits[0:10]) . tensor([[-0.0937, -0.0978, -0.0586, 0.0161, 0.0557, 0.0227, -0.0226, -0.0067, 0.1092, -0.1295], [-0.0878, -0.0855, -0.0318, 0.0267, 0.0569, 0.0349, -0.0275, 0.0007, 0.0999, -0.1215], [-0.0829, -0.1012, -0.0541, 0.0155, 0.0562, 0.0162, -0.0258, -0.0100, 0.1077, -0.1310], [-0.1004, -0.0744, -0.0163, 0.0465, 0.0382, 0.0289, -0.0569, 0.0015, 0.1003, -0.1266], [-0.0946, -0.0994, -0.0636, 0.0132, 0.0539, 0.0236, -0.0221, -0.0034, 0.1013, -0.1298], [-0.0922, -0.0974, -0.0334, 0.0369, 0.0622, 0.0378, -0.0497, 0.0005, 0.0983, -0.1160], [-0.0834, -0.0942, -0.0414, 0.0258, 0.0573, 0.0174, -0.0218, -0.0105, 0.1045, -0.1307], [-0.0782, -0.0985, -0.0458, 0.0154, 0.0579, 0.0214, -0.0227, -0.0060, 0.1035, -0.1269], [-0.1019, -0.0963, -0.0549, 0.0214, 0.0551, 0.0203, -0.0167, -0.0048, 0.1131, -0.1316], [-0.1078, -0.1001, -0.0372, 0.0187, 0.0682, 0.0412, -0.0265, -0.0021, 0.1033, -0.1191]], grad_fn=&lt;SliceBackward&gt;) . Training . Now let&#39;s look at the code for training the image classification model. But first, let&#39;s declare a few helper functions needed to train the model: . import torch.optim as optim # Device device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # Model instance model = ImageRNN(BATCH_SIZE, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) def get_accuracy(logit, target, batch_size): &#39;&#39;&#39; Obtain accuracy for training round &#39;&#39;&#39; corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum() accuracy = 100.0 * corrects/batch_size return accuracy.item() . Before training a model in PyTorch, you can programatically specify what device you want to use during training; the torch.device(...) function tells the program that we want to use the GPU if one is available, otherwise the CPU will be the default device. . Then we create an instance of the model, ImageRNN(...)``, with the proper parameters. The criterion represents the function we will use to compute the loss of the model. Thenn.CrossEntropyLoss()` function basically applies a log softmax followed by a negative log likelihood loss operation over the output of the model. To compute the loss, the function needs both the log probabilities and targets. We will see later in our code how to provide this to the criterion. . For training, we also need an optimization algorithm which helps to update weights based on the current loss. This is achieved with the optim.Adam optimization function, which requires the model parameters and a learning rate. Alternatively, you can also use optim.SGD or any other optimization algorithm that&#39;s available. . The get_accuracy(...) function simply computes the accuracy of the model given the log probabilities and target values. As an exercise, you can write code to test this function as we did with the model before. . Let&#39;s put everything together and train our image classification model: . for epoch in range(N_EPHOCS): # loop over the dataset multiple times train_running_loss = 0.0 train_acc = 0.0 model.train() # TRAINING ROUND for i, data in enumerate(trainloader): # zero the parameter gradients optimizer.zero_grad() # reset hidden states model.hidden = model.init_hidden() # get the inputs inputs, labels = data inputs = inputs.view(-1, 28,28) # forward + backward + optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() train_running_loss += loss.detach().item() train_acc += get_accuracy(outputs, labels, BATCH_SIZE) model.eval() print(&#39;Epoch: %d | Loss: %.4f | Train Accuracy: %.2f&#39; %(epoch, train_running_loss / i, train_acc/i)) . Epoch: 0 | Loss: 0.7489 | Train Accuracy: 75.88 Epoch: 1 | Loss: 0.3113 | Train Accuracy: 91.05 Epoch: 2 | Loss: 0.2325 | Train Accuracy: 93.33 Epoch: 3 | Loss: 0.1957 | Train Accuracy: 94.53 Epoch: 4 | Loss: 0.1706 | Train Accuracy: 95.21 Epoch: 5 | Loss: 0.1564 | Train Accuracy: 95.58 Epoch: 6 | Loss: 0.1471 | Train Accuracy: 95.91 Epoch: 7 | Loss: 0.1329 | Train Accuracy: 96.14 Epoch: 8 | Loss: 0.1283 | Train Accuracy: 96.42 Epoch: 9 | Loss: 0.1196 | Train Accuracy: 96.65 . We can also compute accuracy on the testing dataset to test how well the model performs on the image classification task. As you can see below, our RNN model is performing very well on the MNIST classification task. . test_acc = 0.0 for i, data in enumerate(testloader, 0): inputs, labels = data inputs = inputs.view(-1, 28, 28) outputs = model(inputs) test_acc += get_accuracy(outputs, labels, BATCH_SIZE) print(&#39;Test Accuracy: %.2f&#39;%( test_acc/i)) . Test Accuracy: 95.83 . Final Words . Please notice that we are not using GPU in this tutorial since the models we are building are relatively simple. As an exercise, you can take a look at the PyTorch documentation to learn how to program specific operations to execute on the GPU. You can then try to optimize the code to run on the GPU. If you need help with this, reach out to me on Twitter. . That&#39;s it for this tutorial. Congratulations! You are now able to implement a basic RNN in PyTorch. You also learned how to apply RNNs to solve a real-world, image classification problem. . In the next tutorial, we will do more advanced things with RNNs and try to solve even more complex problems, such as sarcasm detection and sentiment classification. Until next time! . References . A Simple Neural Network from Scratch with PyTorch and Google Colab | Hands on Machine Learning with Scikit-learn and Tensorflow | .",
            "url": "https://dair-ai.github.io/notebooks/neural%20network/beginner/pytorch/rnn/2020/03/18/RNN_PT.html",
            "relUrl": "/neural%20network/beginner/pytorch/rnn/2020/03/18/RNN_PT.html",
            "date": " • Mar 18, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "A Gentle Introduction to PyTorch 1.2",
            "content": "About . In our previous PyTorch notebook, we learned about how to get started quickly with PyTorch 1.2 using Google Colab. In this tutorial, we are going to take a step back and review some of the basic components of building a deep learning model using PyTorch. . This will be a brief tutorial and will avoid using jargon and overcomplicated code. That said, this is perhaps the most basic of models you can build with PyTorch. . If fact, it is so basic that it&#39;s ideal for those starting to learn about PyTorch and deep learning. So if you have a friend or colleague that wants to jump in, I highly encourage you to refer them to this tutorial as a starting point. Let&#39;s get started! . Author: Elvis Saravia . Complete Code Walkthrough: Blog post . . Getting Started . Before getting started, we need to import a few modules which will be useful to obtain the necessary functions that will help us to build our deep learning model. The main ones are torch and torchvision. They contain the majority of the functions that you need to get started with PyTorch. However, as this is a deep learning tutorial we will need torch.nn, torch.nn.functional and torchvision.transforms which all contain utility functions to build our model. We probably won&#39;t use all the modules listed below but they are the typical modules you will be importing when starting your deep learning projects. . ## The usual imports import torch import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms ## for printing image import matplotlib.pyplot as plt import numpy as np . Below we check for the PyTorch version just to make sure that you are using the proper version. At the time of this tutorial, we are working with PyTorch 1.2. . ## print out the pytorch version used print(torch.__version__) . 1.4.0 . Loading the Data . Let&#39;s get right into it! As with any machine learning project, you need to load your dataset. We are using the MNIST dataset, which is the Hello World of datasets in the machine learning world. . The data consists of number images that are of size 28 X 28. We will discuss the images shortly, but our plan is to load data into batches of size 32, similar to the figure below. . . Here are the complete steps we are performing when importing our data: . We will import and tranform the data into tensors using the transforms module | We will use DataLoader to build convenient data loaders, which makes it easy to efficiently feed data in batches to deep learning models. We will get to the topic of batches in a bit but for now just think of them as subsets of your data. | As hinted above, we will also create batches of the data by setting the batch parameter inside the data loader. Notice we use batches of 32 in this tutorial but you can change it to 64 if you like. | . %%capture ## parameter denoting the batch size BATCH_SIZE = 32 ## transformations transform = transforms.Compose( [transforms.ToTensor()]) ## download and load training dataset trainset = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) ## download and load testing dataset testset = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2) . Let&#39;s inspect what the trainset and testset objects contain. . ## print the trainset and testset print(trainset) print(testset) . Dataset MNIST Number of datapoints: 60000 Root location: ./data Split: Train StandardTransform Transform: Compose( ToTensor() ) Dataset MNIST Number of datapoints: 10000 Root location: ./data Split: Test StandardTransform Transform: Compose( ToTensor() ) . This is a beginner&#39;s tutorial so I will break down things a bit here: . BATCH_SIZE is a parameter that denotes the batch size we will use for our model | transform holds code for whatever transformations you will apply to your data. I will show you an example below to demonstrate exactly what it does to shed more light into its use | trainset and testset contain the actual dataset object. Notice I use train=True to specify that this corresponds to the training dataset, and I use train=False to specify that this is the remainder of the dataset which we call the testset. From the portion I printed above you can see that the split of the data was 85% (60000) / 15% (10000), corresponding to the portions of samples for training set and testing set, respectively. | trainloader is what holds the data loader object which takes care of shuffling the data and constructing the batches. | . Now let&#39;s look at that transforms.Compose(...) function and see what it does. We will use a randomized image to demonstrate its use. Let&#39;s generate an image. . image = transforms.ToPILImage(mode=&#39;L&#39;)(torch.randn(1, 96, 96)) . And let&#39;s render it: . plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x7f2ff27d6588&gt; . Okay, we have our image sample. And now let&#39;s apply some dummy transformation to it. We are going to rotate the image by 45 degrees. The transformation below takes care of that: . ## dummy transformation dummy_transform = transforms.Compose( [transforms.RandomRotation(45, fill=(0,))]) dummy_result = dummy_transform(image) plt.imshow(dummy_result) . &lt;matplotlib.image.AxesImage at 0x7f2ff22f5b00&gt; . Notice you can put the transformations within transforms.Compose(...). You can use the built in transformations offered by PyTorch or you can build your own and compose as you wish. In fact, you can place as many transformation as you wish in there. Let&#39;s try another composition of transformations: rotate + vertical flip. . ## dummy transform dummy2_transform = transforms.Compose( [transforms.RandomRotation(45, fill=(0,)), transforms.RandomVerticalFlip()]) dummy2_result = dummy2_transform(image) plt.imshow(dummy2_result) . &lt;matplotlib.image.AxesImage at 0x7f2ff112a358&gt; . That&#39;s pretty cool right! Keep trying other transform methods. On the topic of exploring our data further, let&#39;s take a look at our images dataset. . Exploring the Data . As a practioner and researcher, I am always spend a bit of time and effort exploring and understanding my datasets. It&#39;s fun and this is a good practise to ensure that everything is in order. . Let&#39;s check what the train and test dataset contain. I will use matplotlib to print out some of the images from our dataset. With a bit of numpy I can convert images into numpy and print them out. Below I print out an entire batch. . ## functions to show an image def imshow(img): #img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) ## get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() ## show images imshow(torchvision.utils.make_grid(images)) . The dimensions of our batches are as follow: . for images, labels in trainloader: print(&quot;Image batch dimensions:&quot;, images.shape) print(&quot;Image label dimensions:&quot;, labels.shape) break . Image batch dimensions: torch.Size([32, 1, 28, 28]) Image label dimensions: torch.Size([32]) . The Model . Now it&#39;s time to build the deep learning model that will be used to perform the image classification. We will keeps things simple and stack a few dense layers and a dropout layer to train our model. . Let&#39;s discuss a bit about the model: . First of all the following structure involving a class is standard code that&#39;s used to build the neural network model in PyTorch: | . class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() # layers go here def forward(self, x): # computations go here . The layers are defined inside def __init__(). super(...).__init__() is just there to stick things together. For our model, we stack a hidden layer (self.d1) followed by a dropout layer (self.dropout), which is then followed by an output layer (self.d2). | nn.Linear(...) defines the dense layer and it requires the in and out dimensions, which corresponds to the size of the input feature and output feature of that layer, respectively. | nn.Dropout(...) is used to define a dropout layer. Dropout is an approach in deep learning that helps a model to avoid overfitting. This means that dropout acts as a regularization technique that helps the model to not overfit on the images it has seen while training. We want this because we need a model that generalizes well to unseen examples -- in our case, the testing dataset. Dropout randomly zeroes some of the units of the neural network layer with probability of p=0.2. Read more about the dropout layer here. | The entry point of the model, i.e. where the data enters, is placed under the forward(...) function. Typically, we also place other transformations we perform on the data while training inside this function. | In the forward() function we are performing a series of computations on the input data we flatten the images first, converting it from 2D (28 X 28) to 1D (1 X 784). | then we feed the batches of those 1D images into the first hidden layer | the output of that hidden layer is then applied a non-linear activate function) called ReLU. It&#39;s not so important to know what F.relu() does at the moment, but the effect that it achieves is that it allows faster and more effective training of neural architectures on large datasets | as explained above, the dropout also helps the model to train more efficiently by avoiding overfitting on the training data | we then feed the output of that dropout layer into the output layer (d2) | the result of that is then fed to the softmax function, which converts or normalized the output into a probability distribution which helps with outputting proper predictions values that are used to calculate the accuracy of the model; this will the final output of the model | . | . ## the model class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.d1 = nn.Linear(28 * 28, 128) self.dropout = nn.Dropout(p=0.2) self.d2 = nn.Linear(128, 10) def forward(self, x): x = x.flatten(start_dim = 1) x = self.d1(x) x = F.relu(x) x = self.dropout(x) logits = self.d2(x) out = F.softmax(logits, dim=1) return out . Visually, the following is a diagram of the model we have built. Just keep in mind that the hidden layer is much bigger as shown in the diagram but due to space constraint, the diagram is just an approximation to the actual model. . . As I have done in my previous tutorials, I always encourage to test the model with 1 batch to ensure that the output dimensions are what we expect. Notice how we are iterating over the dataloader which conveniently stores the images and labels pairs. out contains the output of the model, which are the logits applied a softmax layer which helps with prediction. . ## test the model with 1 batch model = MyModel() for images, labels in trainloader: print(&quot;batch size:&quot;, images.shape) out = model(images) print(out.shape) break . batch size: torch.Size([32, 1, 28, 28]) torch.Size([32, 10]) . We can clearly see that we get back the batches with 10 output values associate with it. These are used to compute the performance of the model. . Training the Model . Now we are ready to train the model but before that we are going to setup a loss function, an optimizer and a function to compute accuracy of the model. . The learning_rate is the rate at which the model will try to optimize its weights, which is just another parameter for the model. | num_epochs is the number of training steps. | device determines what hardware we will use to train the model. If a gpu is present, then that will be used, otherwise it defaults to the cpu. | model is just the model instance. | model.to(device) is in charge of setting the actaull device that will be used for training the model | criterion is just the metric that&#39;s used to compute the loss of the model while it forward and backward trains to optimize its weights. | optimizer is the optimization technique used to modify the weights in the backward propagation. Notice that it requires the learning_rate and the model parameters which are part of the calculation to optimize weights. | . learning_rate = 0.001 num_epochs = 5 device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) model = MyModel() model = model.to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) . The utility function below helps to calculate the accuracy of the model. For now, it&#39;s not important to understand how it&#39;s calculated but basically it compares the outputs of the model (predictions) with the actual target values (i.e., the labels of the dataset), and tries to compute the average of correct predictions. . ## utility function to compute accuracy def get_accuracy(output, target, batch_size): &#39;&#39;&#39; Obtain accuracy for training round &#39;&#39;&#39; corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).sum() accuracy = 100.0 * corrects/batch_size return accuracy.item() . Training the Model . Now it&#39;s time to train the model. The code portion that follows can be descrive in the following steps: . The first thing in training a neural network model is defining the training loop, which is achieved by: | . for epoch in range(num_epochs): ... . We define two variables, training_running_loss and train_acc that will help us to monitor the running accuracy and loss of the modes while it trains over the different batches. | model.train() explicitly indicates that we are ready to start training. | Notice how we are iterating over the dataloader, which conveniently gives us the batches in image-label pairs. | That second for loop means that for every training step we will iterate over all the batches and train the model over them. | We feed the model the images via model(images) and the output are the predictions of the model. | The predictions together with the target labels are used to compute the loss using the loss function we defined earlier. | Before we update our weights for the next round of training, we perform the following steps: . we use the optimizer object to reset all the gradients for the variables it will update. This is a safe step and it doesn&#39;t overwrites the gradients the model accumulates while training (those are stored in a buffer link text via the `loss.backward() call) | loss.backward() simply computes the gradient of the loss w.r.t to the model parameters | optimizer.step() then ensures that the model parameters are updated | . | Then we gather and accumulate the loss and accuracy, which is what we will use to tell us if the model is learning properly . | . ## train the model for epoch in range(num_epochs): train_running_loss = 0.0 train_acc = 0.0 ## commence training model = model.train() ## training step for i, (images, labels) in enumerate(trainloader): images = images.to(device) labels = labels.to(device) ## forward + backprop + loss predictions = model(images) loss = criterion(predictions, labels) optimizer.zero_grad() loss.backward() ## update model params optimizer.step() train_running_loss += loss.detach().item() train_acc += get_accuracy(predictions, labels, BATCH_SIZE) model.eval() print(&#39;Epoch: %d | Loss: %.4f | Train Accuracy: %.2f&#39; %(epoch, train_running_loss / i, train_acc/i)) . Epoch: 0 | Loss: 1.5956 | Train Accuracy: 88.89 Epoch: 1 | Loss: 1.5311 | Train Accuracy: 93.71 Epoch: 2 | Loss: 1.5156 | Train Accuracy: 95.17 Epoch: 3 | Loss: 1.5072 | Train Accuracy: 95.87 Epoch: 4 | Loss: 1.5019 | Train Accuracy: 96.34 . After all the training steps are over, we can clearly see that the loss keeps decreasing while the training accuracy of the model keeps rising, which is a good sign that the model is effectively learning to classify images. . We can verify that by computing the accuracy on the testing dataset to see how well the model performs on the image classificaiton task. As you can see below, our basic CNN model is performing very well on the MNIST classification task. . test_acc = 0.0 for i, (images, labels) in enumerate(testloader, 0): images = images.to(device) labels = labels.to(device) outputs = model(images) test_acc += get_accuracy(outputs, labels, BATCH_SIZE) print(&#39;Test Accuracy: %.2f&#39;%( test_acc/i)) . Test Accuracy: 97.03 . Final Words . Congratulation! You have made it to the end of this tutorial. This is a really long tutorial that aims to give an very basic introduction to the fundamentals of image classification using neural networks and PyTorch. . This tutorial was heavily inspired by this TensorFlow tutorial. We thank the authors of the corresponding reference for their valuable work. . References . PyTorch 1.2 Quickstart with Google Colab | Get started with TensorFlow 2.0 for beginners | PyTorch Data Loading Tutorial - Neural Networks with PyTorch | .",
            "url": "https://dair-ai.github.io/notebooks/pytorch/beginner/deep%20learning/2020/03/18/A_Gentle_Introduction_to_PyTorch_1_2.html",
            "relUrl": "/pytorch/beginner/deep%20learning/2020/03/18/A_Gentle_Introduction_to_PyTorch_1_2.html",
            "date": " • Mar 18, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "dair.ai is a community effort to democratize Artificial Intelligence (AI) research, education, and technologies. . Notebooks is an effort to encourage data scientists from all levels to easily share their notebooks. .",
          "url": "https://dair-ai.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}