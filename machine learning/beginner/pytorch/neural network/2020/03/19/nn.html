<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A Simple Neural Network from Scratch with PyTorch and Google Colab | Notebooks by dair.ai</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="A Simple Neural Network from Scratch with PyTorch and Google Colab" />
<meta name="author" content="Elvis Saravia" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this tutorial we implement a simple neural network from scratch using PyTorch." />
<meta property="og:description" content="In this tutorial we implement a simple neural network from scratch using PyTorch." />
<link rel="canonical" href="https://dair-ai.github.io/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html" />
<meta property="og:url" content="https://dair-ai.github.io/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html" />
<meta property="og:site_name" content="Notebooks by dair.ai" />
<meta property="og:image" content="https://dair-ai.github.io/notebooks/images/nn.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-19T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Elvis Saravia"},"description":"In this tutorial we implement a simple neural network from scratch using PyTorch.","headline":"A Simple Neural Network from Scratch with PyTorch and Google Colab","dateModified":"2020-03-19T00:00:00-05:00","datePublished":"2020-03-19T00:00:00-05:00","@type":"BlogPosting","image":"https://dair-ai.github.io/notebooks/images/nn.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://dair-ai.github.io/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html"},"url":"https://dair-ai.github.io/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notebooks/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://dair-ai.github.io/notebooks/feed.xml" title="Notebooks by dair.ai" /><link rel="shortcut icon" type="image/x-icon" href="/notebooks/images/favicon.png">
<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A Simple Neural Network from Scratch with PyTorch and Google Colab | Notebooks by dair.ai</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="A Simple Neural Network from Scratch with PyTorch and Google Colab" />
<meta name="author" content="Elvis Saravia" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this tutorial we implement a simple neural network from scratch using PyTorch." />
<meta property="og:description" content="In this tutorial we implement a simple neural network from scratch using PyTorch." />
<link rel="canonical" href="https://dair-ai.github.io/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html" />
<meta property="og:url" content="https://dair-ai.github.io/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html" />
<meta property="og:site_name" content="Notebooks by dair.ai" />
<meta property="og:image" content="https://dair-ai.github.io/notebooks/images/nn.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-19T00:00:00-05:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Elvis Saravia"},"description":"In this tutorial we implement a simple neural network from scratch using PyTorch.","headline":"A Simple Neural Network from Scratch with PyTorch and Google Colab","dateModified":"2020-03-19T00:00:00-05:00","datePublished":"2020-03-19T00:00:00-05:00","@type":"BlogPosting","image":"https://dair-ai.github.io/notebooks/images/nn.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://dair-ai.github.io/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html"},"url":"https://dair-ai.github.io/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://dair-ai.github.io/notebooks/feed.xml" title="Notebooks by dair.ai" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notebooks/">Notebooks by dair.ai</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notebooks/about/">About</a><a class="page-link" href="/notebooks/search/">Search</a><a class="page-link" href="/notebooks/https:/github.com/dair-ai/notebooks#how-to-contribute">Submit Notebook</a><a class="page-link" href="/notebooks/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">A Simple Neural Network from Scratch with PyTorch and Google Colab</h1><p class="page-description">In this tutorial we implement a simple neural network from scratch using PyTorch.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-03-19T00:00:00-05:00" itemprop="datePublished">
        Mar 19, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Elvis Saravia</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notebooks/categories/#machine learning">machine learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notebooks/categories/#beginner">beginner</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notebooks/categories/#pytorch">pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notebooks/categories/#neural network">neural network</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/dair-ai/notebooks/tree/master/_notebooks/2020-03-19-nn.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/notebooks/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/dair-ai/notebooks/master?filepath=_notebooks%2F2020-03-19-nn.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/notebooks/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/dair-ai/notebooks/blob/master/_notebooks/2020-03-19-nn.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/notebooks/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-03-19-nn.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="About">About<a class="anchor-link" href="#About"> </a></h2><p>In this tutorial we will implement a simple neural network from scratch using PyTorch. The idea of the tutorial is to teach you the basics of PyTorch and how it can be used to implement a neural network from scratch. I will go over some of the basic functionalities and concepts available in PyTorch that will allow you to build your own neural networks.</p>
<p>This tutorial assumes you have prior knowledge of how a neural network works. Don’t worry! Even if you are not so sure, you will be okay. For advanced PyTorch users, this tutorial may still serve as a refresher. This tutorial is heavily inspired by this <a href="https://repl.it/talk/announcements/Build-a-Neural-Network-in-Python/5457">Neural Network implementation</a> coded purely using Numpy. In fact, I tried re-implementing the code using PyTorch instead and added my own intuitions and explanations. Thanks to <a href="https://repl.it/@shamdasani">Samay</a> for his phenomenal work, I hope this inspires many others as it did with me.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>torch</code> module provides all the necessary <strong>tensor</strong> operators you will need to implement your first neural network from scratch in PyTorch. That's right! In PyTorch everything is a Tensor, so this is the first thing you will need to get used to. Let's import the libraries we will need for this tutorial.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Data">Data<a class="anchor-link" href="#Data"> </a></h2><p>Let's start by creating some sample data using the <code>torch.tensor</code> command. In Numpy, this could be done with <code>np.array</code>. Both functions serve the same purpose, but in PyTorch everything is a Tensor as opposed to a vector or matrix. We define types in PyTorch using the <code>dtype=torch.xxx</code> command.</p>
<p>In the data below, <code>X</code> represents the amount of hours studied and how much time students spent sleeping, whereas <code>y</code> represent grades. The variable <code>xPredicted</code> is a single input for which we want to predict a grade using the parameters learned by the neural network. Remember, the neural network wants to learn a mapping between <code>X</code> and <code>y</code>, so it will try to take a guess from what it has learned from the training data.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="c1"># 3 X 2 tensor</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(([</span><span class="mi">92</span><span class="p">],</span> <span class="p">[</span><span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="mi">89</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="c1"># 3 X 1 tensor</span>
<span class="n">xPredicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="c1"># 1 X 2 tensor</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can check the size of the tensors we have just created with the <code>size</code> command. This is equivalent to the <code>shape</code> command used in tools such as Numpy and Tensorflow.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>torch.Size([3, 2])
torch.Size([3, 1])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Scaling">Scaling<a class="anchor-link" href="#Scaling"> </a></h2><p>Below we are performing some scaling on the sample data. Notice that the <code>max</code> function returns both a tensor and the corresponding indices. So we use <code>_</code> to capture the indices which we won't use here because we are only interested in the max values to conduct the scaling. Perfect! Our data is now in a very nice format our neural network will appreciate later on.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># scale units</span>
<span class="n">X_max</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">xPredicted_max</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">xPredicted</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_max</span><span class="p">)</span>
<span class="n">xPredicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">xPredicted</span><span class="p">,</span> <span class="n">xPredicted_max</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">/</span> <span class="mi">100</span>  <span class="c1"># max test score is 100</span>
<span class="nb">print</span><span class="p">(</span><span class="n">xPredicted</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([0.5000, 1.0000])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that there are two functions <code>max</code> and <code>div</code> that I didn't discuss above. They do exactly what they imply: <code>max</code> finds the maximum value in a vector... I mean tensor; and <code>div</code> is basically a nice little function to divide two tensors.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Model-(Computation-Graph)">Model (Computation Graph)<a class="anchor-link" href="#Model-(Computation-Graph)"> </a></h2><p>Once the data has been processed and it is in the proper format, all you need to do now is to define your model. Here is where things begin to change a little as compared to how you would build your neural networks using, say, something like Keras or Tensorflow. However, you will realize quickly as you go along that PyTorch doesn't differ much from other deep learning tools. At the end of the day we are constructing a computation graph, which is used to dictate how data should flow and what type of operations are performed on this information.</p>
<p>For illustration purposes, we are building the following neural network or computation graph:</p>
<p><img src="https://drive.google.com/uc?export=view&amp;id=1l-sKpcCJCEUJV1BlAqcVAvLXLpYCInV6" alt="alt text" /></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Neural_Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Neural_Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># parameters</span>
        <span class="c1"># TODO: parameters can be parameterized instead of declaring them here</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputSize</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">outputSize</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSize</span> <span class="o">=</span> <span class="mi">3</span>
        
        <span class="c1"># weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSize</span><span class="p">)</span> <span class="c1"># 3 X 2 tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputSize</span><span class="p">)</span> <span class="c1"># 3 X 1 tensor</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">)</span> <span class="c1"># 3 X 3 &quot;.dot&quot; does not broadcast in PyTorch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">)</span> <span class="c1"># activation function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">)</span>
        <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z3</span><span class="p">)</span> <span class="c1"># final activation function</span>
        <span class="k">return</span> <span class="n">o</span>
        
    <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">s</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">sigmoidPrime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="c1"># derivative of sigmoid</span>
        <span class="k">return</span> <span class="n">s</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">s</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">o</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">o</span> <span class="c1"># error in output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_error</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoidPrime</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="c1"># derivative of sig to error</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z2_error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o_delta</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z2_delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z2_error</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoidPrime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">z2_delta</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_delta</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># forward + backward pass for training</span>
        <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">o</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">saveWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
        <span class="c1"># we will use the PyTorch internal storage functions</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;NN&quot;</span><span class="p">)</span>
        <span class="c1"># you can reload model with all the weights and so forth with:</span>
        <span class="c1"># torch.load(&quot;NN&quot;)</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Predicted data based on trained weights: &quot;</span><span class="p">)</span>
        <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Input (scaled): </span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">xPredicted</span><span class="p">))</span>
        <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Output: </span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">xPredicted</span><span class="p">)))</span>
        
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the purpose of this tutorial, we are not going to be talking math stuff, that's for another day. I just want you to get a gist of what it takes to build a neural network from scratch using PyTorch. Let's break down the model which was declared via the class above.</p>
<h2 id="Class-Header">Class Header<a class="anchor-link" href="#Class-Header"> </a></h2><p>First, we defined our model via a class because that is the recommended way to build the computation graph. The class header contains the name of the class <code>Neural Network</code> and the parameter <code>nn.Module</code> which basically indicates that we are defining our own neural network.</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Neural_Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</pre></div>
<h2 id="Initialization">Initialization<a class="anchor-link" href="#Initialization"> </a></h2><p>The next step is to define the initializations ( <code>def __init__(self,)</code>) that will be performed upon creating an instance of the customized neural network. You can declare the parameters of your model here, but typically, you would declare the structure of your network in this section -- the size of the hidden layers and so forth. Since we are building the neural network from scratch, we explicitly declared the size of the weights matrices: one that stores the parameters from the input to hidden layer; and one that stores the parameter from the hidden to output layer. Both weight matrices are initialized with values randomly chosen from a normal distribution via <code>torch.randn(...)</code>. Note that we are not using bias just to keep things as simple as possible.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Neural_Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># parameters</span>
    <span class="c1"># TODO: parameters can be parameterized instead of declaring them here</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inputSize</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">outputSize</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSize</span> <span class="o">=</span> <span class="mi">3</span>

    <span class="c1"># weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hiddenSize</span><span class="p">)</span> <span class="c1"># 3 X 2 tensor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hiddenSize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputSize</span><span class="p">)</span> <span class="c1"># 3 X 1 tensor</span>
</pre></div>
<h2 id="The-Forward-Function">The Forward Function<a class="anchor-link" href="#The-Forward-Function"> </a></h2><p>The <code>forward</code> function is where all the magic happens (see below). This is where the data enters and is fed into the computation graph (i.e., the neural network structure we have built). Since we are building a simple neural network with one hidden layer, our forward function looks very simple:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">)</span> 
    <span class="bp">self</span><span class="o">.</span><span class="n">z2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">)</span> <span class="c1"># activation function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">z3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">)</span>
    <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z3</span><span class="p">)</span> <span class="c1"># final activation function</span>
    <span class="k">return</span> <span class="n">o</span>
</pre></div>
<p>The <code>forward</code> function above takes the input <code>X</code>and then performs a matrix multiplication (<code>torch.matmul(...)</code>) with the first weight matrix <code>self.W1</code>. Then the result is applied an activation function, <code>sigmoid</code>. The resulting matrix of the activation is then multiplied with the second weight matrix <code>self.W2</code>. Then another activation if performed, which renders the output of the neural network or computation graph. The process I described above is simply what's known as a <code>feedforward pass</code>. In order for the weights to optimize when training, we need a backpropagation algorithm.</p>
<h2 id="The-Backward-Function">The Backward Function<a class="anchor-link" href="#The-Backward-Function"> </a></h2><p>The <code>backward</code> function contains the backpropagation algorithm, where the goal is to essentially minimize the loss with respect to our weights. In other words, the weights need to be updated in such  a way that the loss decreases while the neural network is training (well, that is what we hope for). All this magic is possible with the gradient descent algorithm which is declared in the <code>backward</code> function. Take a minute or two to inspect what is happening in the code below:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">o</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">o_error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">o</span> <span class="c1"># error in output</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">o_delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_error</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoidPrime</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> 
    <span class="bp">self</span><span class="o">.</span><span class="n">z2_error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">o_delta</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">z2_delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">z2_error</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoidPrime</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">z2_delta</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">t</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_delta</span><span class="p">)</span>
</pre></div>
<p>Notice that we are performing a lot of matrix multiplications along with the transpose operations via the <code>torch.matmul(...)</code> and <code>torch.t(...)</code> operations, respectively. The rest is simply gradient descent -- there is nothing to it.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training">Training<a class="anchor-link" href="#Training"> </a></h2><p>All that is left now is to train the neural network. First we create an instance of the computation graph we have just built:</p>
<div class="highlight"><pre><span></span><span class="n">NN</span> <span class="o">=</span> <span class="n">Neural_Network</span><span class="p">()</span>
</pre></div>
<p>Then we train the model for <code>1000</code> rounds. Notice that in PyTorch <code>NN(X)</code> automatically calls the <code>forward</code> function so there is no need to explicitly call <code>NN.forward(X)</code>.</p>
<p>After we have obtained the predicted output for ever round of training, we compute the loss, with the following code:</p>
<div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">NN</span><span class="p">(</span><span class="n">X</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
<p>The next step is to start the training (foward + backward) via <code>NN.train(X, y)</code>. After we have trained the neural network, we can store the model and output the predicted value of the single instance we declared in the beginning, <code>xPredicted</code>.</p>
<p>Let's train!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">NN</span> <span class="o">=</span> <span class="n">Neural_Network</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>  <span class="c1"># trains the NN 1,000 times</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">100</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;#&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; Loss: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">NN</span><span class="p">(</span><span class="n">X</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>  <span class="c1"># mean sum squared loss</span>
    <span class="n">NN</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">NN</span><span class="o">.</span><span class="n">saveWeights</span><span class="p">(</span><span class="n">NN</span><span class="p">)</span>
<span class="n">NN</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Finished training!&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>#0 Loss: 0.24544493854045868
#100 Loss: 0.0026628002524375916
#200 Loss: 0.0024748605210334063
#300 Loss: 0.002363199135288596
#400 Loss: 0.0022466194350272417
#500 Loss: 0.0021235516760498285
#600 Loss: 0.001996910898014903
#700 Loss: 0.0018705682596191764
#800 Loss: 0.0017485078424215317
#900 Loss: 0.0016340742586180568
Predicted data based on trained weights: 
Input (scaled): 
tensor([0.5000, 1.0000])
Output: 
tensor([0.9529])
Finished training!
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn&#39;t retrieve source code for container of type Neural_Network. It won&#39;t be checked for correctness upon loading.
  &#34;type &#34; + obj.__name__ + &#34;. It won&#39;t be checked &#34;
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The loss keeps decreasing, which means that the neural network is learning something. That's it. Congratulations! You have just learned how to create and train a neural network from scratch using PyTorch. There are so many things you can do with the shallow network we have just implemented. You can add more hidden layers or try to incorporate the bias terms for practice. I would love to see what you will build from here. Reach me out on <a href="https://twitter.com/omarsar0">Twitter</a> if you have any further questions or leave your comments here. Until next time!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References:">References:<a class="anchor-link" href="#References:"> </a></h2><ul>
<li><a href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules">PyTorch nn. Modules</a></li>
<li><a href="https://enlight.nyc/neural-network">Build a Neural Network with Numpy</a></li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="dair-ai/notebooks"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/notebooks/machine%20learning/beginner/pytorch/neural%20network/2020/03/19/nn.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notebooks/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/notebooks/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notebooks/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Sharing data science notebooks made easy.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/dair-ai" title="dair-ai"><svg class="svg-icon grey"><use xlink:href="/notebooks/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/dair_ai" title="dair_ai"><svg class="svg-icon grey"><use xlink:href="/notebooks/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
